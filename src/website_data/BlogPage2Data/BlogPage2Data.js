import React, { useState, Suspense, useRef, useEffect, useMemo } from 'react';
import { faBars, faSection } from '@fortawesome/free-solid-svg-icons';
import Plot from 'react-plotly.js';

// Importing blog data overview and necessary components
import { footer_content, blogpage_subtitle_content, blogpage_title_content, blogpage_references_content } from '../GeneralBlogPageData';

// Import figures
import Figure1 from './images/Figure1.png'
import Figure2 from './images/Figure2.png'
import Figure3 from './images/Figure3.png'
import Figure4 from './images/Figure4.png'

// Import tSNE and CM Components
import tsne_data from './data/TSNE_Data.json'
import cm_data from './data/CM_Data.json'

// Robustness Plot Components
function RobustnessPlot() {
    // X-axis: 1024 down to 128 with intervals of 16
    // We generate this automatically to save space
    const xValues = [];
    for (let i = 1024; i >= 128; i -= 16) {
        xValues.push(i);
    }

    const robustnessData = {
        'SSG': [98.7, 98.7, 98.7, 98.7, 97.8, 95.8, 93.30000000000001, 90.60000000000001, 89.1, 87.1, 83.5, 79.7, 75.0, 69.6, 63.6, 57.099999999999994, 51.9, 46.5, 42.6, 39.6, 37.1, 34.699999999999996, 33.5, 31.6, 29.799999999999997, 28.199999999999996, 26.400000000000002, 24.7, 22.7, 20.9, 19.400000000000002, 18.0, 17.0, 16.0, 15.0, 14.2, 14.000000000000002, 13.900000000000002, 14.099999999999998, 13.900000000000002, 14.099999999999998, 13.900000000000002, 13.600000000000001, 12.3, 11.0, 9.6, 8.5, 8.0, 7.6, 7.5, 7.3999999999999995, 7.199999999999999, 7.199999999999999, 6.800000000000001, 6.6000000000000005, 6.3, 6.1],
        'MSG': [98.8, 98.8, 98.6, 97.5, 96.7, 95.7, 95.19999999999999, 94.3, 94.1, 93.30000000000001, 92.80000000000001, 91.8, 90.60000000000001, 89.8, 87.5, 85.2, 82.1, 78.2, 74.0, 67.7, 62.0, 56.10000000000001, 51.0, 45.6, 41.6, 36.7, 33.300000000000004, 29.5, 26.5, 23.5, 20.7, 17.7, 16.3, 14.499999999999998, 13.200000000000001, 11.700000000000001, 10.6, 9.5, 8.799999999999999, 7.9, 7.6, 6.6000000000000005, 6.0, 5.3, 5.1, 5.0, 4.9, 5.0, 5.0, 5.4, 5.6000000000000005, 5.3, 5.1, 4.9, 4.6, 4.3, 3.9],
        'MRG': [98.6, 98.5, 98.6, 98.6, 97.1, 95.6, 93.4, 90.7, 88.4, 86.0, 83.39999999999999, 80.4, 76.7, 72.7, 68.10000000000001, 63.1, 57.8, 53.400000000000006, 49.1, 44.4, 40.300000000000004, 36.199999999999996, 32.4, 28.9, 25.4, 22.6, 20.3, 18.2, 16.0, 14.799999999999999, 13.8, 13.0, 12.4, 11.4, 11.1, 10.5, 10.2, 9.8, 9.3, 9.1, 8.9, 8.4, 7.9, 7.5, 7.000000000000001, 6.2, 5.4, 4.8, 4.3, 4.2, 4.2, 4.2, 4.2, 4.2, 4.2, 4.2, 4.2],
        'SSG+DP': [96.5, 96.6, 96.7, 96.5, 96.6, 96.39999999999999, 96.39999999999999, 96.39999999999999, 96.5, 96.39999999999999, 96.5, 96.3, 96.39999999999999, 96.2, 96.2, 96.3, 96.2, 96.1, 96.1, 95.89999999999999, 96.0, 96.1, 96.0, 95.8, 95.89999999999999, 95.7, 95.5, 95.3, 95.5, 95.19999999999999, 95.3, 95.1, 94.89999999999999, 94.89999999999999, 94.8, 94.39999999999999, 94.3, 93.8, 93.7, 93.10000000000001, 92.7, 92.5, 91.60000000000001, 91.2, 90.60000000000001, 89.7, 88.9, 87.7, 86.2, 84.8, 82.89999999999999, 80.10000000000001, 77.7, 73.6, 69.3, 64.9, 59.599999999999994],
        'MSG+DP': [97.8, 97.8, 97.7, 97.7, 97.8, 97.8, 97.8, 97.6, 97.7, 97.7, 97.8, 97.8, 97.8, 97.6, 97.7, 97.8, 97.7, 97.7, 97.6, 97.6, 97.5, 97.39999999999999, 97.39999999999999, 97.5, 97.39999999999999, 97.39999999999999, 97.5, 97.39999999999999, 97.2, 97.2, 97.3, 97.2, 97.0, 97.0, 96.7, 96.5, 96.6, 96.2, 96.1, 96.1, 96.0, 95.6, 95.5, 95.1, 94.69999999999999, 94.3, 94.0, 93.5, 93.10000000000001, 91.9, 91.3, 90.3, 89.2, 88.1, 86.4, 83.89999999999999, 81.5],
        'MRG+DP': [96.6, 96.6, 96.6, 96.7, 96.6, 96.5, 96.7, 96.39999999999999, 96.6, 96.6, 96.6, 96.7, 96.6, 96.5, 96.5, 96.5, 96.39999999999999, 96.5, 96.3, 96.39999999999999, 96.39999999999999, 96.39999999999999, 96.3, 96.39999999999999, 96.2, 96.0, 96.0, 96.2, 96.1, 96.0, 95.8, 95.7, 95.5, 95.3, 95.1, 95.0, 94.8, 94.69999999999999, 94.19999999999999, 93.8, 93.60000000000001, 93.30000000000001, 92.7, 92.10000000000001, 91.5, 90.60000000000001, 90.2, 89.3, 88.1, 87.0, 85.2, 83.2, 81.10000000000001, 78.9, 76.2, 72.7, 67.9],
    };

    // Helper to create trace
    const createTrace = (name, color, data) => ({
        x: xValues,
        y: data,
        type: 'scatter',
        mode: 'lines+markers',
        name: name,
        line: { color: color, width: 2 },
        marker: { size: 4 },

        hovertemplate: '%{fullData.name}: %{y:.2f}%<extra></extra>'
    });

    const plotData = [
        createTrace('SSG', '#ffb6c1', robustnessData['SSG']),      // Light Pink
        createTrace('MSG', '#90ee90', robustnessData['MSG']),      // Light Green
        createTrace('MRG', '#add8e6', robustnessData['MRG']),      // Light Blue
        createTrace('SSG+DP', '#8b0000', robustnessData['SSG+DP']), // Dark Red
        createTrace('MSG+DP', '#006400', robustnessData['MSG+DP']), // Dark Green
        createTrace('MRG+DP', '#00008b', robustnessData['MRG+DP']), // Dark Blue
    ];

    return (
        <div className="robustness-container">
            <Plot
                data={plotData}
                layout={{
                    autosize: true,
                    title: 'Model Robustness vs. Point Density',
                    paper_bgcolor: 'rgba(0,0,0,0)',
                    plot_bgcolor: 'rgba(0,0,0,0)',
                    font: { color: 'lightgray' },
                    xaxis: { 
                        title: {
                            text: 'Number of Points', 
                            standoff: 10
                        },
                        autorange: 'reversed', // High density (left) to low density (right)
                        gridcolor: 'rgba(255,255,255,0.1)',
                        unifiedhovertitle: { text: 'Point %{x}' }
                    },
                    yaxis: {
                        title: {
                            text: 'Accuracy (%)',
                            standoff: 10
                        },
                        range: [0, 100],
                        tickformat: '.0f',
                        ticksuffix: '%',
                        gridcolor: 'rgba(255,255,255,0.1)'
                    },
                    legend: {
                        orientation: 'h',
                        x: 0.5,
                        xanchor: 'center',
                        // y: 0,
                        xref: 'paper',
                        yref: 'paper',
                        bgcolor: 'rgba(0,0,0,0)'
                    },
                    hoverlabel: {
                        bgcolor: 'rgba(0,0,0,0)',     // transparent
                        bordercolor: 'rgba(0,0,0,0)', // no border
                        font: { color: 'lightgray' }
                    },
                    margin: { l: 70, r: 20, b: 50, t: 50, pad: 4 },
                    hovermode: 'x unified' // Shows all values at a specific x-point on hover
                }}
                useResizeHandler={true}
                style={{ width: '100%', height: '100%' }}
                config={{ displayModeBar: false }}
            />
        </div>
    );
}

// TSNE Plot Components
const nameMap = { ssg: 'SSG', ssgdp: 'SSG+DP', msg: 'MSG', msgdp: 'MSG+DP', mrg: 'MRG', mrgdp: 'MRG+DP' };

function buildDataMap(jsonData) {
    const map = {};

    Object.entries(jsonData).forEach(([key, value]) => {
    // key format: tsne_ssg_128 OR cm_ssg_128
    const parts = key.split('_'); // ["tsne", "ssg", "128"] or ["cm", "ssg", "128"]

    const rawModel = parts[1]; // "ssg", "ssgdp", "msg", etc.
    const dim = Number(parts[2]);

    const model = nameMap[rawModel];

    if (!map[model]) map[model] = {};
        map[model][dim] = value;
    });

    return map;
}

const tsneDataMap = buildDataMap(tsne_data);
const cmDataMap = buildDataMap(cm_data);

const CADTypes = [
    'O-Ring',  'Through Hole',  'Blind Hole',  'Triangular Passage',
    'Rectangular Passage', 'Circular Through Slot', 'Triangular Through Slot', 'Rectangular Through Slot',
    'Rectangular Blind Slot', 'Triangular Pocket', 'Rectangular Pocket', 'Circular End Pocket',
    'Triangular Blind Step', 'Circular Blind Step', 'Rectangular Blind Step', 'Rectangular Through Step',
    '2-Sides Through Step', 'Slanted Through Step', 'Chamfer', 'Round',
    'Vertical Circular End Blind Slot', 'Horizontal Circular End Blind Slot', '6-Sides Passage', '6-Sides Pocket',
];

function TSNE() {
    const [selectedModel, setSelectedModel] = useState('MSG+DP');
    const [selectedPoints, setSelectedPoints] = useState(1024);

    const [showLegend, setShowLegend] = useState(window.innerWidth > 768);

    useEffect(() => {
        const handleResize = () => {
            // Update state based on width threshold
            setShowLegend(window.innerWidth > 768);
        };

        window.addEventListener('resize', handleResize);
        
        // Cleanup listener on component unmount
        return () => window.removeEventListener('resize', handleResize);
    }, []);

    const plotData = useMemo(() => {
        const groups = {};
        const activeData = tsneDataMap[selectedModel][selectedPoints];

        if (!activeData) return [];

        activeData.forEach(point => {
            const label = point.label;
            if (!groups[label]) {
                groups[label] = {
                    name: CADTypes[label],
                    type: 'scatter3d',
                    mode: 'markers',
                    x: [], y: [], z: [],
                    marker: { size: 3, opacity: 0.7 }
                };
            }
            groups[label].x.push(point.x);
            groups[label].y.push(point.y);
            groups[label].z.push(point.z);
        });

        return Object.values(groups);
    }, [selectedModel, selectedPoints]);

    const models = ['SSG', 'SSG+DP', 'MSG', 'MSG+DP', 'MRG', 'MRG+DP'];
    const pointOptions = [128, 256, 384, 512, 640, 768, 896, 1024];

    return (
        // CHANGE HERE: flexDirection: 'row' puts them side-by-side
        <>
            <div className="tsne-container" >
                <Plot
                    data={plotData}
                    layout={{
                        autosize: true,
                        showLegend: showLegend,

                        hoverlabel: {
                            namelength: -1, // -1 means "show the full name, do not cut it off"
                        },

                        paper_bgcolor: 'rgba(0,0,0,0)',
                        plot_bgcolor: 'rgba(0,0,0,0)',
                        font: { color: 'lightgray' },
                        scene: {
                            xaxis: { title: 'Dim 1' },
                            yaxis: { title: 'Dim 2' },
                            zaxis: { title: 'Dim 3' },
                            aspectmode: 'cube',
                            domain: { 
                                x: [showLegend ? 0.25 : 0, 1], 
                                y: [0.1, 1] 
                            }
                        },
                        margin: { l: 0, r: 0, b: 0, t: 0, pad: 0 },
                        // Legend settings for compact view inside the plot area
                        legend: { x: 0, y: 0.5, font: { size: 10 } }
                    }}
                    config={{
                        modeBarButtons: [
                            ['zoom3d', 'pan3d', 'resetCameraDefault3d', 'orbitRotation', 'tableRotation'] 
                        ]
                    }}
                    useResizeHandler={true}
                    style={{ width: '100%', height: '100%' }}
                />
            </div>

            <div className="TSNE_Selector">
                
                {/* 1. Model Selector */}
                <div className="TSNE_ModelSelector">
                    <p className="Text">Model Variant</p>
                    <div>
                        {models.map(model => (
                            <label key={model}>
                                <input 
                                    type="radio" 
                                    name="model" 
                                    value={model} 
                                    checked={selectedModel === model} 
                                    onChange={() => setSelectedModel(model)}
                                />
                                {model}
                            </label>
                        ))}
                    </div>
                </div>

                <hr style={{ width: '100%', borderColor: 'rgba(255,255,255,0.1)' }} />

                {/* 2. Points Selector */}
                <div className="TSNE_PointsSelector">
                    <p className="Text">Point Density</p>
                    <div>
                        {pointOptions.map(points => (
                            <label key={points}>
                                <input 
                                    type="radio" 
                                    name="points" 
                                    value={points} 
                                    checked={selectedPoints === parseInt(points)} // Ensure type match
                                    onChange={() => setSelectedPoints(parseInt(points))}
                                />
                                {points} Points
                            </label>
                        ))}
                    </div>
                </div>

            </div>
        </>
    );
}

function ConfusionMatrix() {
    const [selectedModel, setSelectedModel] = useState('MSG+DP');
    const [selectedPoints, setSelectedPoints] = useState(1024);

    const matrix = useMemo(() => {
        return cmDataMap[selectedModel][selectedPoints] || [];
    }, [selectedModel, selectedPoints]);

    // Must match your Python class label ordering
    const classNames = CADTypes;

    const models = ['SSG', 'SSG+DP', 'MSG', 'MSG+DP', 'MRG', 'MRG+DP'];
    const pointOptions = [128, 256, 384, 512, 640, 768, 896, 1024];

    return (
        <>
            <div className="cm-container">
                <Plot
                    data={[
                        {
                            z: matrix,
                            x: classNames,
                            y: classNames,
                            type: 'heatmap',
                            colorscale: 'RdBu',
                            zmin: 0,
                            zmax: 1,
                            text: matrix.map(row => row.map(v => v.toFixed(1))), 
                            texttemplate: "%{text}",
                            textfont: { color: "white" },
                            hoverongaps: false,
                            hovertemplate: "Predicted Class: %{x}<br>True Class: %{y}<br>Accuracy: %{z:.2%}<extra></extra>"
                        }
                    ]}

                    layout={{
                        autosize: true,
                        paper_bgcolor: 'rgba(0,0,0,0)',
                        plot_bgcolor: 'rgba(0,0,0,0)',
                        font: { color: 'lightgray' },

                        xaxis: {
                            title: {
                                text: "Predicted Class",
                                standoff: 10      // adds space between axis and heatmap
                            },
                            showticklabels: false,
                            fixedrange: true
                        },
                        yaxis: {
                            title: {
                                text: "True Class",
                                standoff: 10
                            },
                            autorange: "reversed",
                            showticklabels: false,
                            fixedrange: true
                        },

                        margin: { l: 40, r: 0, b: 40, t: 20 },

                        coloraxis: { colorbar: { title: "Accuracy" } }
                    }}
                    config={{
                        displayModeBar: true,
                        dragmode: false,
                        modeBarButtonsToRemove: [
                            "pan2d",
                            "select2d",
                            "lasso2d",
                            "autoScale2d",
                            "hoverClosestCartesian",
                            "hoverCompareCartesian",
                            "zoom3d",
                            "pan3d",
                            "orbitRotation",
                            "tableRotation",
                            "toImage",
                            "resetCameraLastSave",
                            "toggleSpikelines",
                            "hoverClosestGl2d",
                            "hoverClosestGeo",
                            "hoverClosestGl3d",
                            "hoverClosestPie",
                            "textEditing",
                            "sendDataToCloud"
                        ]
                    }}
                    useResizeHandler={true}
                    style={{ width: '100%', height: '100%' }}
                />
            </div>

            <div className="CM_Selector">

                <div className="CM_ModelSelector">
                    <p className="Text">Model Variant</p>
                    <div>
                        {models.map(model => (
                            <label key={model}>
                                <input
                                    type="radio"
                                    name="model"
                                    value={model}
                                    checked={selectedModel === model}
                                    onChange={() => setSelectedModel(model)}
                                />
                                {model}
                            </label>
                        ))}
                    </div>
                </div>

                <hr style={{ width: '100%', borderColor: 'rgba(255,255,255,0.1)' }} />

                <div className="CM_PointsSelector">
                    <p className="Text">Point Density</p>
                    <div>
                        {pointOptions.map(points => (
                            <label key={points}>
                                <input
                                    type="radio"
                                    name="points"
                                    value={points}
                                    checked={selectedPoints === points}
                                    onChange={() => setSelectedPoints(points)}
                                />
                                {points} Points
                            </label>
                        ))}
                    </div>
                </div>

            </div>
        </>
    );
}

// Robustness Plot Components
function TimeTaken() {
    const timetakenData = {
        'SSG': {
            'x': [0.0, 0.04555555555555556, 0.0961111111111111, 0.15194444444444444, 0.2075, 0.2813888888888889, 0.3638888888888889, 0.45111111111111113, 0.49777777777777776, 0.5511111111111111, 0.6127777777777778, 0.6763888888888889, 0.7394444444444445, 0.8041666666666667, 0.8683333333333333, 0.9327777777777778, 0.9972222222222222, 1.061388888888889, 1.1269444444444445, 1.1916666666666667, 1.2561111111111112, 1.3216666666666668, 1.3858333333333333, 1.4491666666666667, 1.513611111111111, 1.5775, 1.6425, 1.7069444444444444, 1.7697222222222222, 1.8341666666666667, 1.8977777777777778, 1.9622222222222223, 2.0283333333333333, 2.091388888888889, 2.1558333333333333, 2.221111111111111, 2.2847222222222223, 2.3491666666666666, 2.4133333333333336, 2.4766666666666666, 2.538611111111111, 2.601111111111111, 2.664722222222222, 2.7294444444444443, 2.7930555555555556, 2.8575, 2.9225, 2.9875, 3.0516666666666667, 3.1169444444444445, 3.1830555555555557, 3.2472222222222222, 3.3119444444444444, 3.3766666666666665, 3.4408333333333334, 3.5025, 3.562777777777778, 3.624722222222222, 3.685277777777778, 3.7466666666666666, 3.806388888888889, 3.8669444444444445, 3.928888888888889, 3.99, 4.051944444444445, 4.113611111111111, 4.175, 4.2363888888888885, 4.2975, 4.358611111111111, 4.421666666666667, 4.4825, 4.543611111111111, 4.605555555555555, 4.665277777777778, 4.7252777777777775, 4.786944444444444, 4.848888888888889, 4.911111111111111, 4.973333333333334, 5.0344444444444445, 5.096666666666667, 5.158333333333333, 5.220833333333333, 5.2813888888888885, 5.343333333333334, 5.404166666666667, 5.463888888888889, 5.5247222222222225, 5.585555555555556, 5.6475, 5.708055555555555, 5.768055555555556, 5.830555555555556, 5.892222222222222, 5.953611111111111, 6.016388888888889, 6.078055555555555, 6.140833333333333, 6.2025, 6.2636111111111115, 6.325833333333334, 6.388055555555556, 6.4494444444444445, 6.512777777777778, 6.573888888888889, 6.636666666666667, 6.6975, 6.759166666666666, 6.822222222222222, 6.884166666666666, 6.946111111111111, 7.009166666666666, 7.068333333333333, 7.130277777777778, 7.190555555555555, 7.251944444444445, 7.313888888888889, 7.3741666666666665, 7.433333333333334, 7.494722222222222, 7.555, 7.616944444444444, 7.6786111111111115, 7.740277777777778, 7.800833333333333, 7.8625, 7.9238888888888885, 7.984166666666667, 8.034166666666666, 8.081944444444444, 8.135833333333334, 8.191666666666666, 8.254444444444445, 8.31861111111111, 8.380277777777778, 8.441666666666666, 8.502777777777778, 8.563333333333333, 8.626944444444444, 8.688333333333333, 8.750277777777777, 8.812777777777777, 8.873888888888889, 8.934722222222222, 8.99611111111111, 9.057222222222222, 9.12, 9.181666666666667, 9.24388888888889, 9.305277777777778, 9.365277777777777, 9.426944444444445, 9.486666666666666, 9.546944444444444, 9.607222222222223, 9.66611111111111, 9.733055555555556, 9.78888888888889, 9.85361111111111, 9.912777777777778, 10.02, 10.106111111111112, 10.210277777777778, 10.335833333333333, 10.455277777777777, 10.581388888888888, 10.708333333333334, 10.83361111111111, 10.95861111111111, 11.078611111111112, 11.14, 11.199444444444444, 11.258611111111112, 11.319444444444445, 11.37861111111111, 11.436111111111112, 11.49388888888889, 11.551666666666666, 11.61111111111111, 11.667777777777777, 11.7275, 11.784444444444444, 11.844166666666666, 11.902222222222223, 11.950277777777778, 11.999722222222223, 12.055555555555555, 12.115277777777777, 12.173055555555555, 12.229444444444445, 12.285833333333333, 12.345, 12.405833333333334, 12.465277777777779, 12.525833333333333, 12.5875, 12.649722222222222, 12.6975, 12.762777777777778, 12.822777777777778, 12.886944444444444, 12.949444444444444, 13.010555555555555, 13.071666666666667, 13.131666666666666, 13.19138888888889, 13.251944444444444, 13.3125, 13.37111111111111, 13.430833333333334, 13.490833333333333, 13.551666666666666, 13.610277777777778, 13.67, 13.723333333333333, 13.767777777777777, 13.814444444444444, 13.864166666666666, 13.921666666666667, 13.982222222222223, 14.042222222222222, 14.101666666666667, 14.161388888888888, 14.221944444444444, 14.281666666666666, 14.342777777777778, 14.403055555555556, 14.461666666666666, 14.52111111111111, 14.583333333333334, 14.645, 14.704444444444444, 14.764166666666666, 14.825, 14.885, 14.946111111111112, 15.007222222222222, 15.065833333333334, 15.126388888888888, 15.186944444444444, 15.247777777777777, 15.30888888888889, 15.3675, 15.428611111111111, 15.487777777777778, 15.547777777777778, 15.608055555555556, 15.661388888888888, 15.717222222222222, 15.771944444444445],
            'y': [53.800000000000004, 63.7, 74.0, 69.19999999999999, 79.0, 74.2, 84.89999999999999, 83.1, 87.5, 84.5, 87.6, 86.8, 90.8, 90.2, 91.7, 90.7, 90.7, 91.5, 91.60000000000001, 88.5, 91.3, 92.4, 92.30000000000001, 91.60000000000001, 94.19999999999999, 93.2, 93.7, 94.6, 94.89999999999999, 94.1, 93.2, 94.5, 95.8, 95.0, 95.5, 93.7, 95.5, 95.5, 94.8, 94.8, 93.7, 94.39999999999999, 94.5, 94.0, 95.89999999999999, 96.1, 95.8, 96.5, 96.5, 96.1, 96.3, 96.6, 96.2, 95.5, 96.6, 97.3, 96.89999999999999, 96.8, 96.8, 97.0, 96.5, 95.5, 97.0, 97.1, 96.6, 97.0, 97.3, 91.60000000000001, 96.2, 97.1, 96.5, 97.2, 97.7, 96.7, 96.7, 96.6, 97.5, 97.8, 97.3, 97.6, 96.8, 97.39999999999999, 97.5, 97.3, 97.8, 97.8, 97.8, 96.3, 97.0, 97.39999999999999, 97.7, 97.39999999999999, 97.7, 97.5, 97.6, 97.7, 97.8, 96.89999999999999, 97.5, 97.2, 97.6, 97.8, 96.5, 97.7, 97.3, 97.6, 97.0, 97.0, 97.3, 97.5, 97.1, 97.8, 97.6, 97.7, 97.8, 98.0, 98.1, 97.1, 97.7, 97.2, 97.7, 97.89999999999999, 97.8, 96.5, 97.39999999999999, 97.6, 97.39999999999999, 97.8, 97.39999999999999, 98.0, 98.2, 97.7, 97.8, 97.8, 97.8, 97.1, 97.5, 97.7, 98.0, 97.5, 97.1, 97.8, 98.0, 97.39999999999999, 98.0, 97.5, 97.8, 98.0, 98.1, 97.8, 96.3, 97.6, 98.1, 97.8, 97.6, 97.6, 97.7, 97.6, 97.3, 98.1, 97.8, 97.2, 97.39999999999999, 97.7, 97.8, 98.0, 98.1, 98.0, 98.4, 97.2, 97.7, 97.8, 98.4, 98.3, 97.6, 97.8, 97.6, 98.1, 98.4, 97.7, 97.89999999999999, 97.6, 97.89999999999999, 98.1, 98.2, 97.5, 98.3, 97.89999999999999, 97.39999999999999, 97.3, 97.6, 98.0, 97.5, 97.8, 97.6, 97.8, 98.0, 98.1, 97.8, 97.2, 98.3, 98.0, 97.8, 98.0, 98.1, 98.0, 98.1, 98.2, 97.3, 98.0, 98.2, 98.0, 97.89999999999999, 98.1, 97.6, 98.0, 98.3, 97.3, 97.3, 97.3, 97.89999999999999, 97.89999999999999, 97.6, 97.8, 97.8, 98.2, 97.89999999999999, 97.8, 97.6, 97.6, 97.8, 98.0, 97.3, 97.6, 97.39999999999999, 97.6, 97.5, 98.4, 98.0, 98.0, 97.8, 97.89999999999999, 98.1, 97.89999999999999, 98.1, 98.0, 98.0, 97.6, 98.0, 98.2, 97.7]
        },

        'MSG': {
            'x': [0.0, 0.09444444444444444, 0.18611111111111112, 0.2775, 0.36777777777777776, 0.45944444444444443, 0.5519444444444445, 0.6433333333333333, 0.7347222222222223, 0.8266666666666667, 0.9194444444444444, 1.011111111111111, 1.1016666666666666, 1.1936111111111112, 1.2861111111111112, 1.378611111111111, 1.4697222222222222, 1.5616666666666668, 1.6533333333333333, 1.7455555555555555, 1.8366666666666667, 1.928611111111111, 2.020277777777778, 2.1127777777777776, 2.203611111111111, 2.295, 2.3872222222222224, 2.4788888888888887, 2.5694444444444446, 2.661388888888889, 2.7530555555555556, 2.845, 2.9363888888888887, 3.0275, 3.12, 3.2116666666666664, 3.3005555555555555, 3.39, 3.481388888888889, 3.5716666666666668, 3.6633333333333336, 3.755, 3.8466666666666667, 3.937777777777778, 4.029166666666667, 4.121111111111111, 4.213333333333333, 4.304166666666666, 4.396111111111111, 4.487222222222222, 4.579166666666667, 4.67, 4.761666666666667, 4.853611111111111, 4.9463888888888885, 5.0377777777777775, 5.13, 5.223055555555556, 5.316111111111111, 5.408611111111111, 5.500277777777778, 5.593055555555556, 5.686388888888889, 5.800277777777778, 5.96, 6.0938888888888885, 6.230555555555555, 6.3725, 6.554722222222222, 6.760555555555555, 6.964166666666666, 7.163055555555555, 7.3613888888888885, 7.504166666666666, 7.598611111111111, 7.693333333333333, 7.788333333333333, 7.884444444444444, 7.979722222222223, 8.074166666666667, 8.169444444444444, 8.265, 8.359722222222222, 8.455277777777777, 8.5525, 8.648333333333333, 8.743333333333334, 8.83611111111111, 8.931111111111111, 9.026111111111112, 9.120833333333334, 9.216388888888888, 9.311944444444444, 9.407222222222222, 9.501944444444444, 9.5975, 9.708055555555555, 9.790833333333333, 9.884444444444444, 9.975277777777778, 10.144722222222223, 10.290277777777778, 10.436666666666667, 10.535277777777777, 10.629166666666666, 10.726111111111111, 10.838611111111112, 10.915555555555555, 10.986666666666666, 11.068055555555556, 11.144166666666667, 11.227222222222222, 11.365555555555556, 11.503055555555555, 11.6375, 11.771944444444445, 11.9025, 11.984722222222222, 12.064722222222223, 12.149166666666666, 12.234166666666667, 12.3225, 12.407222222222222, 12.491944444444444, 12.577777777777778, 12.6625, 12.747777777777777, 12.831944444444444, 12.917777777777777, 13.008055555555556, 13.0975, 13.19111111111111, 13.285, 13.379444444444445, 13.472777777777777, 13.56611111111111, 13.659444444444444, 13.753333333333334, 13.843333333333334, 13.935833333333333, 14.028888888888888, 14.122777777777777, 14.216111111111111, 14.309444444444445, 14.4025, 14.496944444444445, 14.59, 14.684166666666666, 14.7775, 14.869722222222222, 14.963888888888889, 15.057222222222222, 15.151388888888889, 15.245277777777778, 15.339166666666667, 15.4325, 15.525833333333333, 15.62, 15.713611111111112, 15.806111111111111, 15.899722222222222, 15.993333333333334, 16.087222222222223, 16.1775, 16.27027777777778, 16.363888888888887, 16.456944444444446, 16.550833333333333, 16.64388888888889, 16.73638888888889, 16.83, 16.923333333333332, 17.0175, 17.108333333333334, 17.201944444444443, 17.295833333333334, 17.39027777777778, 17.48472222222222, 17.578611111111112, 17.671666666666667, 17.765, 17.858055555555556, 17.951666666666668, 18.044722222222223, 18.137777777777778, 18.231944444444444, 18.325555555555557, 18.419444444444444, 18.5125, 18.60527777777778, 18.698611111111113, 18.79222222222222, 18.886111111111113, 18.97888888888889, 19.07138888888889, 19.165277777777778, 19.259166666666665, 19.35333333333333, 19.447222222222223, 19.54083333333333, 19.63388888888889, 19.728055555555557, 19.821944444444444, 19.91638888888889, 20.009166666666665, 20.1025, 20.19638888888889, 20.290277777777778, 20.384722222222223, 20.477777777777778, 20.570833333333333, 20.664722222222224, 20.75861111111111, 20.850555555555555, 20.94472222222222, 21.037777777777777, 21.13222222222222, 21.225833333333334, 21.32027777777778, 21.415, 21.50777777777778, 21.601666666666667, 21.695555555555554, 21.844722222222224, 22.004722222222224, 22.14611111111111, 22.235555555555557, 22.321944444444444, 22.411666666666665, 22.501666666666665, 22.592222222222222, 22.684722222222224, 22.77722222222222, 22.87, 22.961388888888887, 23.05361111111111, 23.151944444444446, 23.245833333333334, 23.338333333333335, 23.431944444444444, 23.525277777777777, 23.61861111111111, 23.711111111111112, 23.801666666666666, 23.889166666666668, 23.976944444444445, 24.06361111111111, 24.149444444444445, 24.236666666666668, 24.320833333333333, 24.40277777777778],
            'y': [41.0, 57.699999999999996, 77.3, 76.2, 83.1, 85.9, 85.1, 85.9, 87.8, 88.1, 88.3, 91.10000000000001, 91.4, 92.0, 89.3, 91.5, 92.9, 94.0, 93.60000000000001, 94.8, 95.0, 94.8, 93.7, 96.2, 94.3, 95.89999999999999, 96.1, 96.0, 95.6, 97.0, 95.5, 96.39999999999999, 96.6, 96.6, 96.89999999999999, 96.6, 96.39999999999999, 96.6, 96.5, 97.0, 96.0, 97.1, 97.0, 97.39999999999999, 96.7, 96.7, 97.8, 97.3, 96.89999999999999, 97.2, 97.1, 96.5, 97.5, 97.2, 97.5, 96.5, 97.1, 97.89999999999999, 97.2, 97.3, 97.2, 97.8, 97.6, 98.0, 97.89999999999999, 98.1, 97.5, 98.0, 97.6, 98.1, 97.7, 97.7, 97.5, 98.0, 97.89999999999999, 97.1, 98.3, 97.7, 97.6, 97.7, 97.5, 98.2, 97.7, 98.3, 98.3, 98.1, 97.0, 98.2, 98.3, 98.1, 97.89999999999999, 98.0, 98.2, 98.4, 98.5, 98.2, 97.89999999999999, 97.5, 98.0, 98.1, 98.2, 97.6, 97.89999999999999, 98.0, 98.3, 97.89999999999999, 97.89999999999999, 97.89999999999999, 98.1, 98.2, 98.1, 98.3, 97.7, 98.0, 98.1, 98.3, 98.4, 97.8, 98.0, 98.3, 97.89999999999999, 97.8, 98.1, 98.3, 98.4, 98.0, 98.5, 98.4, 98.4, 98.1, 98.4, 98.1, 98.0, 98.2, 98.4, 97.3, 98.4, 98.4, 98.5, 98.2, 98.2, 98.1, 98.4, 98.3, 98.6, 97.89999999999999, 98.2, 98.0, 98.1, 98.6, 98.0, 97.8, 98.5, 98.6, 98.4, 98.6, 98.2, 98.3, 98.2, 98.6, 98.3, 98.1, 98.3, 98.2, 98.4, 97.89999999999999, 97.6, 98.2, 98.3, 97.89999999999999, 98.3, 98.4, 98.3, 98.2, 98.5, 98.0, 97.7, 98.1, 98.4, 98.2, 98.4, 98.3, 98.4, 98.0, 98.5, 98.5, 98.0, 98.4, 98.4, 98.4, 98.4, 98.3, 97.8, 98.5, 98.3, 98.4, 98.2, 98.2, 98.2, 98.6, 98.5, 97.6, 98.6, 98.3, 98.4, 98.2, 97.5, 98.5, 98.4, 98.3, 98.5, 98.3, 98.2, 98.6, 98.4, 98.3, 98.6, 98.5, 98.5, 98.0, 98.4, 98.4, 98.1, 98.4, 98.4, 98.4, 98.2, 98.5, 98.1, 97.89999999999999, 98.2, 98.5, 97.8, 98.0, 98.3, 98.4, 98.4, 98.4, 98.7, 98.3, 98.3, 98.0, 98.4, 98.5, 98.5, 98.3, 98.1, 97.89999999999999, 98.2, 98.3, 98.5]
        },

        'MRG': {
            'x': [0.0, 0.05722222222222222, 0.12222222222222222, 0.1875, 0.2513888888888889, 0.31472222222222224, 0.37833333333333335, 0.4411111111111111, 0.5033333333333333, 0.5669444444444445, 0.6302777777777778, 0.6919444444444445, 0.7519444444444444, 0.8133333333333334, 0.8733333333333333, 0.9538888888888889, 1.0338888888888889, 1.1030555555555555, 1.1625, 1.2241666666666666, 1.2875, 1.3508333333333333, 1.4158333333333333, 1.4802777777777778, 1.5452777777777778, 1.6069444444444445, 1.6677777777777778, 1.7297222222222222, 1.79, 1.8502777777777777, 1.9102777777777777, 1.97, 2.026666666666667, 2.0869444444444443, 2.1461111111111113, 2.2075, 2.261388888888889, 2.3041666666666667, 2.3405555555555555, 2.377222222222222, 2.424166666666667, 2.486111111111111, 2.548888888888889, 2.611388888888889, 2.674166666666667, 2.7369444444444446, 2.7994444444444446, 2.8605555555555555, 2.923888888888889, 2.986388888888889, 3.048611111111111, 3.111666666666667, 3.1752777777777776, 3.238888888888889, 3.3002777777777776, 3.3605555555555555, 3.423611111111111, 3.486388888888889, 3.5480555555555555, 3.611666666666667, 3.674722222222222, 3.7375, 3.8002777777777776, 3.861388888888889, 3.923611111111111, 3.987222222222222, 4.0488888888888885, 4.112222222222222, 4.1755555555555555, 4.238611111111111, 4.301388888888889, 4.363333333333333, 4.426111111111111, 4.488611111111111, 4.551111111111111, 4.614444444444445, 4.678333333333334, 4.740555555555556, 4.804166666666666, 4.867222222222222, 4.93, 4.993055555555555, 5.056111111111111, 5.119722222222222, 5.1825, 5.245277777777778, 5.307777777777778, 5.371111111111111, 5.434444444444445, 5.497222222222222, 5.558888888888889, 5.621388888888889, 5.684166666666667, 5.746111111111111, 5.808888888888889, 5.872222222222222, 5.935833333333333, 5.998888888888889, 6.061111111111111, 6.123333333333333, 6.185833333333333, 6.248888888888889, 6.311388888888889, 6.374444444444444, 6.436944444444444, 6.485555555555556, 6.538611111111111, 6.583611111111111, 6.629444444444444, 6.6825, 6.738333333333333, 6.788888888888889, 6.843055555555556, 6.893611111111111, 6.941388888888889, 7.001666666666667, 7.063611111111111, 7.124722222222222, 7.186111111111111, 7.245277777777778, 7.306666666666667, 7.368055555555555, 7.430555555555555, 7.4927777777777775, 7.555, 7.6177777777777775, 7.680833333333333, 7.743888888888889, 7.806666666666667, 7.869166666666667, 7.925833333333333, 7.982777777777778, 8.040833333333333, 8.136944444444444, 8.193888888888889, 8.2525, 8.309166666666666, 8.3675, 8.424444444444445, 8.483888888888888, 8.5425, 8.603055555555555, 8.665555555555555, 8.72638888888889, 8.78888888888889, 8.851111111111111, 8.91388888888889, 8.976111111111111, 9.03888888888889, 9.100833333333334, 9.162777777777778, 9.225277777777778, 9.28111111111111, 9.341666666666667, 9.403888888888888, 9.466944444444444, 9.528333333333334, 9.590555555555556, 9.651944444444444, 9.713888888888889, 9.776944444444444, 9.839444444444444, 9.900833333333333, 9.9625, 10.023055555555555, 10.085555555555555, 10.1475, 10.210555555555555, 10.273055555555555, 10.335833333333333, 10.399166666666666, 10.461666666666666, 10.523333333333333, 10.586388888888889, 10.648333333333333, 10.710833333333333, 10.774166666666666, 10.836944444444445, 10.89861111111111, 10.960277777777778, 11.020555555555555, 11.083055555555555, 11.145277777777778, 11.208055555555555, 11.27111111111111, 11.332222222222223, 11.392777777777777, 11.455277777777777, 11.516944444444444, 11.579722222222221, 11.643055555555556, 11.705277777777777, 11.768611111111111, 11.83, 11.89, 11.952222222222222, 12.014166666666666, 12.0775, 12.140277777777778, 12.201944444444445, 12.26388888888889, 12.325, 12.3875, 12.450277777777778, 12.511944444444444, 12.574444444444444, 12.636388888888888, 12.698333333333334, 12.760555555555555, 12.821666666666667, 12.884444444444444, 12.946944444444444, 13.009722222222223, 13.0725, 13.135277777777778, 13.197222222222223, 13.259166666666667, 13.32, 13.3825, 13.445277777777777, 13.508055555555556, 13.570833333333333, 13.633611111111112, 13.694722222222222, 13.757222222222222, 13.81861111111111, 13.881388888888889, 13.943055555555556, 14.0025, 14.059444444444445, 14.116666666666667, 14.176666666666666, 14.236944444444445, 14.294444444444444, 14.353055555555555, 14.411944444444444, 14.474444444444444, 14.535555555555556, 14.597777777777777, 14.657777777777778, 14.719722222222222, 14.781666666666666, 14.84388888888889, 14.906666666666666, 14.969722222222222, 15.030277777777778, 15.091666666666667, 15.153055555555556, 15.215555555555556, 15.277222222222223, 15.33],
            'y': [41.4, 64.0, 73.9, 74.1, 78.60000000000001, 81.3, 84.2, 88.5, 84.1, 85.8, 89.1, 91.0, 92.0, 92.10000000000001, 91.7, 90.7, 93.10000000000001, 92.9, 92.7, 92.9, 92.4, 94.0, 93.5, 94.3, 92.9, 94.89999999999999, 93.30000000000001, 93.89999999999999, 93.89999999999999, 93.30000000000001, 95.0, 94.3, 94.89999999999999, 95.8, 94.8, 94.89999999999999, 94.3, 95.3, 94.89999999999999, 95.3, 95.8, 96.0, 95.8, 94.6, 95.89999999999999, 96.1, 95.1, 94.8, 96.1, 93.60000000000001, 95.19999999999999, 96.8, 95.1, 97.1, 97.1, 96.39999999999999, 97.39999999999999, 96.1, 96.6, 97.5, 97.2, 96.8, 95.19999999999999, 95.39999999999999, 96.3, 97.6, 97.1, 97.0, 96.2, 97.8, 96.6, 95.5, 97.3, 96.39999999999999, 96.8, 96.39999999999999, 96.39999999999999, 97.39999999999999, 97.1, 95.39999999999999, 96.89999999999999, 97.0, 97.1, 97.3, 97.2, 96.1, 97.39999999999999, 97.5, 98.0, 96.89999999999999, 97.2, 96.89999999999999, 97.89999999999999, 97.39999999999999, 97.8, 97.3, 96.5, 97.2, 96.3, 97.7, 98.0, 97.39999999999999, 97.89999999999999, 97.2, 95.8, 97.6, 97.5, 97.3, 98.0, 97.5, 97.8, 97.5, 97.2, 97.6, 97.6, 98.1, 97.3, 96.7, 97.3, 97.89999999999999, 97.89999999999999, 96.8, 98.0, 96.8, 96.3, 97.0, 97.5, 97.7, 97.89999999999999, 97.6, 97.89999999999999, 97.89999999999999, 96.6, 97.3, 97.3, 97.5, 98.0, 98.0, 97.1, 97.89999999999999, 97.7, 97.7, 97.5, 98.0, 97.7, 98.0, 97.89999999999999, 97.8, 97.2, 97.2, 97.2, 97.0, 97.2, 96.1, 98.2, 98.2, 97.3, 98.0, 96.7, 98.0, 97.5, 97.89999999999999, 97.8, 97.39999999999999, 96.7, 97.6, 97.8, 97.6, 98.1, 97.6, 98.2, 97.89999999999999, 95.6, 97.89999999999999, 97.3, 96.8, 98.1, 97.6, 96.39999999999999, 97.8, 97.89999999999999, 97.5, 96.6, 94.39999999999999, 97.6, 97.8, 96.6, 97.89999999999999, 97.5, 97.89999999999999, 96.8, 97.6, 96.7, 97.89999999999999, 97.89999999999999, 97.3, 96.2, 96.3, 97.5, 97.89999999999999, 97.1, 97.5, 97.2, 97.7, 96.6, 97.89999999999999, 98.2, 97.1, 96.89999999999999, 98.3, 97.3, 96.5, 97.8, 97.6, 98.0, 97.8, 97.0, 97.3, 96.6, 96.5, 95.89999999999999, 97.7, 98.1, 97.39999999999999, 96.8, 98.3, 96.6, 98.1, 97.8, 97.1, 98.0, 96.8, 97.0, 96.8, 98.2, 97.5, 98.2, 97.5, 97.3, 97.0, 97.39999999999999, 97.89999999999999, 97.89999999999999, 97.2, 97.8, 98.0, 96.89999999999999, 97.2, 96.8, 98.0, 97.5]
        },

        'SSG+DP': {
            'x': [0.0, 0.05388888888888889, 0.11861111111111111, 0.17944444444444443, 0.2275, 0.2897222222222222, 0.35444444444444445, 0.42, 0.4855555555555556, 0.5511111111111111, 0.6172222222222222, 0.6836111111111111, 0.7497222222222222, 0.8147222222222222, 0.88, 0.9447222222222222, 1.0108333333333333, 1.0758333333333334, 1.1425, 1.208888888888889, 1.2747222222222223, 1.341388888888889, 1.4077777777777778, 1.4747222222222223, 1.5419444444444443, 1.6080555555555556, 1.6741666666666666, 1.741111111111111, 1.8072222222222223, 1.8725, 1.9375, 2.0033333333333334, 2.068888888888889, 2.1347222222222224, 2.201111111111111, 2.2672222222222222, 2.3338888888888887, 2.4, 2.4652777777777777, 2.5322222222222224, 2.598611111111111, 2.6641666666666666, 2.7311111111111113, 2.7977777777777777, 2.863611111111111, 2.9297222222222223, 2.995833333333333, 3.062222222222222, 3.1280555555555556, 3.193888888888889, 3.2591666666666668, 3.325833333333333, 3.391388888888889, 3.4566666666666666, 3.522777777777778, 3.588611111111111, 3.653611111111111, 3.7194444444444446, 3.785277777777778, 3.850277777777778, 3.915277777777778, 3.9805555555555556, 4.046111111111111, 4.111944444444444, 4.1775, 4.243333333333333, 4.308333333333334, 4.373333333333333, 4.438611111111111, 4.503333333333333, 4.566388888888889, 4.631944444444445, 4.697222222222222, 4.762777777777778, 4.828333333333333, 4.893888888888889, 4.957222222222223, 5.023611111111111, 5.0875, 5.152777777777778, 5.2186111111111115, 5.282777777777778, 5.348055555555556, 5.413055555555555, 5.478611111111111, 5.545277777777778, 5.611111111111111, 5.676111111111111, 5.741388888888889, 5.806111111111111, 5.870555555555556, 5.934722222222222, 6.0, 6.065277777777778, 6.130833333333333, 6.196666666666666, 6.2625, 6.328611111111111, 6.393888888888889, 6.458611111111111, 6.5247222222222225, 6.589444444444444, 6.654722222222222, 6.720277777777778, 6.785555555555556, 6.850555555555555, 6.915277777777778, 6.979722222222223, 7.045555555555556, 7.110555555555556, 7.174722222222222, 7.24, 7.305, 7.368888888888889, 7.433888888888889, 7.498611111111111, 7.562222222222222, 7.6275, 7.6925, 7.7575, 7.823333333333333, 7.886666666666667, 7.939722222222223, 8.009166666666667, 8.069444444444445, 8.13111111111111, 8.186388888888889, 8.235833333333334, 8.291666666666666, 8.369444444444444, 8.466388888888888, 8.549722222222222, 8.654444444444444, 8.771666666666667, 8.886388888888888, 8.992777777777778, 9.10388888888889, 9.2175, 9.331388888888888, 9.446944444444444, 9.562222222222223, 9.66611111111111, 9.779444444444444, 9.897222222222222, 10.006388888888889, 10.119444444444444, 10.225555555555555, 10.281666666666666, 10.336388888888889, 10.390555555555556, 10.445833333333333, 10.495555555555555, 10.537222222222223, 10.579166666666667, 10.623611111111112, 10.664166666666667, 10.721666666666666, 10.766944444444444, 10.809722222222222, 10.85, 10.891944444444444, 10.934444444444445, 10.977777777777778, 11.023333333333333, 11.066666666666666, 11.108611111111111, 11.153611111111111, 11.198055555555555, 11.238888888888889, 11.282777777777778, 11.33361111111111, 11.385, 11.431111111111111, 11.468333333333334, 11.5125, 11.555277777777778, 11.605277777777777, 11.6575, 11.718055555555555, 11.783611111111112, 11.827777777777778, 11.866111111111111, 11.903611111111111, 11.940833333333334, 11.978055555555555, 12.015555555555556, 12.053055555555556, 12.090555555555556, 12.128055555555555, 12.167777777777777, 12.205277777777777, 12.241944444444444, 12.278055555555556, 12.315, 12.354166666666666, 12.391111111111112, 12.428055555555556, 12.464722222222223, 12.50361111111111, 12.562222222222223, 12.615833333333333, 12.673055555555555, 12.73, 12.781666666666666, 12.83388888888889, 12.886111111111111, 12.938333333333333, 12.998333333333333, 13.061111111111112, 13.109722222222222, 13.16388888888889, 13.219444444444445, 13.273888888888889, 13.331111111111111, 13.386666666666667, 13.4425, 13.495555555555555, 13.540277777777778, 13.579166666666667, 13.618055555555555, 13.656944444444445, 13.697222222222223, 13.738888888888889, 13.779444444444444, 13.820555555555556, 13.861666666666666, 13.9025, 13.947777777777778, 13.989166666666666, 14.030833333333334, 14.071388888888889, 14.112777777777778, 14.15638888888889, 14.211944444444445, 14.268055555555556, 14.329444444444444, 14.389166666666666, 14.45, 14.512222222222222, 14.570833333333333, 14.631944444444445, 14.691944444444445, 14.7525, 14.815, 14.874722222222223, 14.935277777777777, 14.994166666666667, 15.055, 15.1175, 15.179444444444444, 15.24],
            'y': [19.6, 23.400000000000002, 39.2, 50.0, 56.599999999999994, 64.9, 69.0, 71.5, 74.5, 78.3, 76.7, 78.60000000000001, 85.39999999999999, 83.0, 85.6, 82.5, 86.2, 83.7, 82.3, 83.3, 82.3, 87.5, 86.3, 88.1, 86.6, 89.0, 89.1, 88.0, 86.8, 88.9, 88.2, 91.0, 89.1, 89.8, 88.9, 91.10000000000001, 90.60000000000001, 91.10000000000001, 89.60000000000001, 91.9, 90.3, 92.0, 88.5, 92.2, 93.30000000000001, 90.60000000000001, 92.30000000000001, 93.60000000000001, 92.0, 91.8, 91.3, 93.60000000000001, 92.0, 92.60000000000001, 92.4, 93.8, 92.10000000000001, 92.4, 91.3, 90.7, 92.30000000000001, 92.9, 92.5, 93.5, 92.7, 93.7, 93.5, 93.60000000000001, 93.60000000000001, 95.19999999999999, 93.4, 93.30000000000001, 94.0, 93.10000000000001, 94.8, 95.19999999999999, 91.10000000000001, 94.69999999999999, 94.5, 93.10000000000001, 94.6, 93.2, 90.7, 92.9, 94.3, 93.4, 91.2, 94.3, 93.89999999999999, 94.6, 94.89999999999999, 92.5, 94.39999999999999, 93.2, 94.19999999999999, 95.39999999999999, 94.8, 94.8, 93.7, 94.8, 93.2, 95.5, 93.4, 93.60000000000001, 94.6, 94.8, 89.0, 92.80000000000001, 95.5, 94.89999999999999, 93.89999999999999, 91.3, 93.89999999999999, 94.39999999999999, 94.8, 93.8, 94.89999999999999, 94.89999999999999, 94.39999999999999, 93.5, 93.8, 95.39999999999999, 94.69999999999999, 95.0, 94.6, 95.1, 94.19999999999999, 94.6, 93.8, 94.1, 89.9, 95.6, 94.69999999999999, 93.8, 91.4, 94.6, 93.4, 95.19999999999999, 93.89999999999999, 94.89999999999999, 94.8, 95.6, 94.39999999999999, 94.39999999999999, 94.89999999999999, 95.3, 94.6, 94.0, 95.39999999999999, 94.19999999999999, 95.3, 95.19999999999999, 95.3, 94.8, 94.1, 94.8, 93.8, 95.1, 93.4, 94.6, 94.8, 95.39999999999999, 94.6, 92.7, 95.19999999999999, 94.89999999999999, 95.0, 92.60000000000001, 94.6, 96.2, 95.6, 95.39999999999999, 94.5, 95.5, 95.5, 94.6, 94.39999999999999, 94.5, 94.5, 90.10000000000001, 95.89999999999999, 94.69999999999999, 94.89999999999999, 93.0, 93.60000000000001, 94.3, 92.10000000000001, 94.69999999999999, 94.89999999999999, 95.1, 94.5, 95.8, 96.0, 95.3, 94.6, 94.1, 92.30000000000001, 93.0, 93.2, 94.8, 95.0, 94.8, 94.19999999999999, 95.39999999999999, 93.5, 95.0, 94.19999999999999, 93.10000000000001, 95.6, 94.19999999999999, 95.3, 94.6, 91.4, 95.6, 96.1, 92.7, 94.89999999999999, 95.39999999999999, 95.0, 95.19999999999999, 95.7, 95.7, 94.3, 93.8, 92.80000000000001, 93.2, 94.6, 92.7, 94.69999999999999, 95.7, 94.1, 95.1, 93.8, 95.3, 95.5, 94.8, 94.5, 94.89999999999999, 94.89999999999999, 93.89999999999999, 94.19999999999999, 95.5, 95.19999999999999, 95.5, 95.7, 95.7, 93.4, 94.8, 95.3, 95.7, 95.1]
        },

        'MSG+DP': {
            'x': [0.0, 0.07222222222222222, 0.15666666666666668, 0.24972222222222223, 0.3358333333333333, 0.42694444444444446, 0.5227777777777778, 0.6188888888888889, 0.7138888888888889, 0.8105555555555556, 0.9066666666666666, 1.0033333333333334, 1.0886111111111112, 1.1652777777777779, 1.2591666666666668, 1.3527777777777779, 1.4447222222222222, 1.531111111111111, 1.625, 1.7161111111111111, 1.8108333333333333, 1.9055555555555554, 2.005, 2.1483333333333334, 2.2958333333333334, 2.4541666666666666, 2.6355555555555554, 2.8333333333333335, 2.9741666666666666, 3.13, 3.223611111111111, 3.3169444444444443, 3.4119444444444444, 3.506388888888889, 3.6013888888888888, 3.6955555555555555, 3.7891666666666666, 3.8830555555555555, 3.9766666666666666, 4.070833333333334, 4.165555555555556, 4.259444444444444, 4.353888888888889, 4.448055555555555, 4.543333333333333, 4.6375, 4.7325, 4.8277777777777775, 4.9222222222222225, 5.015833333333333, 5.11, 5.203611111111111, 5.299166666666666, 5.395555555555555, 5.491388888888889, 5.586944444444445, 5.6819444444444445, 5.778333333333333, 5.895833333333333, 6.0969444444444445, 6.298055555555556, 6.497222222222222, 6.695277777777778, 6.889166666666667, 7.084722222222222, 7.280555555555556, 7.460277777777778, 7.5472222222222225, 7.6338888888888885, 7.765555555555555, 7.880277777777778, 8.02138888888889, 8.157777777777778, 8.286944444444444, 8.41638888888889, 8.543055555555556, 8.639166666666666, 8.713611111111112, 8.785277777777777, 8.868611111111111, 8.979166666666666, 9.091666666666667, 9.211388888888889, 9.316944444444445, 9.480833333333333, 9.598333333333333, 9.678333333333333, 9.759166666666667, 9.868333333333334, 10.010833333333334, 10.145555555555555, 10.280833333333334, 10.415277777777778, 10.5075, 10.600555555555555, 10.696944444444444, 10.789166666666667, 10.883333333333333, 10.978055555555555, 11.073055555555555, 11.168055555555556, 11.262777777777778, 11.357222222222223, 11.451944444444445, 11.546666666666667, 11.640833333333333, 11.735833333333334, 11.831111111111111, 11.925555555555556, 12.019722222222223, 12.113888888888889, 12.20888888888889, 12.304166666666667, 12.399444444444445, 12.494722222222222, 12.588333333333333, 12.68138888888889, 12.776111111111112, 12.870555555555555, 12.965555555555556, 13.061111111111112, 13.155277777777778, 13.249722222222223, 13.344722222222222, 13.439722222222223, 13.535, 13.629444444444445, 13.723333333333333, 13.817222222222222, 13.911944444444444, 14.007222222222222, 14.1025, 14.197777777777778, 14.2925, 14.386944444444444, 14.481388888888889, 14.576666666666666, 14.670833333333333, 14.765833333333333, 14.86138888888889, 14.956111111111111, 15.050277777777778, 15.145, 15.24, 15.335, 15.430277777777778, 15.525555555555556, 15.619722222222222, 15.7125, 15.8075, 15.9025, 15.9975, 16.092222222222222, 16.18638888888889, 16.281388888888888, 16.376666666666665, 16.47222222222222, 16.567222222222224, 16.662222222222223, 16.756666666666668, 16.851111111111113, 16.945833333333333, 17.04111111111111, 17.136111111111113, 17.23138888888889, 17.326944444444443, 17.42111111111111, 17.515555555555554, 17.610277777777778, 17.704166666666666, 17.799444444444443, 17.89472222222222, 17.989444444444445, 18.083333333333332, 18.177777777777777, 18.2725, 18.36722222222222, 18.4625, 18.557222222222222, 18.65, 18.744722222222222, 18.83888888888889, 18.933888888888887, 19.02861111111111, 19.123055555555556, 19.216666666666665, 19.31138888888889, 19.406666666666666, 19.501666666666665, 19.59611111111111, 19.690277777777776, 19.784166666666668, 19.878611111111113, 19.97361111111111, 20.065555555555555, 20.198888888888888, 20.350555555555555, 20.513055555555557, 20.681944444444444, 20.791944444444443, 20.885277777777777, 20.98, 21.074444444444445, 21.169444444444444, 21.264166666666668, 21.35861111111111, 21.452222222222222, 21.546944444444446, 21.64, 21.73361111111111, 21.83277777777778, 21.926388888888887, 22.02, 22.114166666666666, 22.20916666666667, 22.30361111111111, 22.398888888888887, 22.493055555555557, 22.587222222222223, 22.68111111111111, 22.775555555555556, 22.86777777777778, 22.966666666666665, 23.06138888888889, 23.155277777777776, 23.249166666666667, 23.344166666666666, 23.43861111111111, 23.53361111111111, 23.627222222222223, 23.720833333333335, 23.815277777777776, 23.91, 24.004722222222224, 24.099444444444444, 24.194166666666668, 24.289166666666667, 24.38361111111111, 24.47833333333333, 24.573055555555555, 24.66777777777778, 24.763055555555557, 24.857222222222223, 24.95138888888889, 25.046388888888888, 25.140833333333333, 25.235555555555557, 25.330833333333334, 25.424166666666668, 25.518055555555556, 25.612222222222222],
            'y': [20.3, 30.5, 55.300000000000004, 68.2, 71.89999999999999, 76.4, 76.4, 83.0, 77.0, 81.3, 86.4, 87.8, 86.1, 87.4, 87.3, 87.0, 89.60000000000001, 90.4, 89.4, 91.3, 91.9, 92.30000000000001, 90.5, 92.7, 90.2, 92.7, 90.10000000000001, 91.3, 92.0, 92.5, 93.4, 93.8, 92.10000000000001, 92.9, 92.9, 93.89999999999999, 93.5, 93.60000000000001, 91.9, 94.1, 94.69999999999999, 93.7, 94.0, 94.0, 95.3, 94.3, 95.3, 95.3, 95.1, 94.0, 95.1, 95.19999999999999, 94.89999999999999, 93.89999999999999, 95.0, 94.89999999999999, 96.3, 95.19999999999999, 96.39999999999999, 95.5, 95.1, 95.5, 95.39999999999999, 94.6, 96.5, 96.0, 96.5, 95.6, 95.19999999999999, 96.0, 95.7, 96.1, 94.8, 95.8, 95.3, 96.5, 95.89999999999999, 97.1, 96.5, 95.39999999999999, 96.5, 96.3, 96.1, 96.8, 95.19999999999999, 95.7, 96.1, 96.2, 96.7, 95.89999999999999, 96.3, 96.89999999999999, 96.2, 97.1, 96.8, 96.7, 97.3, 96.89999999999999, 95.3, 96.39999999999999, 96.5, 96.7, 96.3, 96.39999999999999, 96.89999999999999, 97.1, 96.39999999999999, 97.0, 96.6, 96.7, 96.8, 96.1, 97.3, 96.39999999999999, 96.8, 97.1, 97.39999999999999, 96.7, 96.1, 96.5, 96.5, 96.5, 96.5, 96.7, 96.89999999999999, 97.2, 96.39999999999999, 96.2, 96.7, 96.39999999999999, 96.8, 95.89999999999999, 96.3, 96.2, 96.8, 96.8, 96.7, 96.6, 97.2, 96.6, 97.3, 96.7, 96.2, 96.89999999999999, 96.5, 96.89999999999999, 97.2, 96.6, 96.7, 97.1, 96.2, 96.89999999999999, 96.89999999999999, 96.7, 96.6, 96.8, 96.6, 96.7, 96.0, 96.39999999999999, 97.2, 97.1, 97.0, 96.6, 96.6, 96.8, 96.6, 96.6, 97.1, 96.39999999999999, 96.89999999999999, 94.89999999999999, 96.3, 96.8, 96.5, 96.5, 96.7, 96.5, 96.89999999999999, 96.89999999999999, 96.5, 97.3, 97.1, 97.0, 96.2, 96.89999999999999, 97.0, 96.7, 97.0, 97.3, 96.89999999999999, 96.8, 97.1, 97.1, 96.8, 96.89999999999999, 96.6, 97.0, 96.7, 96.8, 96.6, 96.0, 97.0, 97.0, 96.7, 96.89999999999999, 96.7, 96.39999999999999, 97.3, 96.89999999999999, 96.8, 96.7, 97.0, 97.1, 96.8, 96.5, 97.39999999999999, 97.1, 96.2, 97.0, 97.0, 96.89999999999999, 97.1, 97.0, 96.8, 97.1, 96.6, 97.2, 96.6, 97.39999999999999, 95.5, 97.2, 96.5, 96.8, 96.8, 96.39999999999999, 97.0, 96.3, 96.0, 96.8, 96.6, 96.89999999999999, 96.7, 97.0, 97.2, 96.89999999999999, 96.5, 97.0, 96.6, 96.89999999999999, 97.3]
        },

        'MRG+DP': {
            'x': [0.0, 0.05611111111111111, 0.11527777777777778, 0.17472222222222222, 0.23527777777777778, 0.2936111111111111, 0.35305555555555557, 0.41305555555555556, 0.4736111111111111, 0.5341666666666667, 0.5919444444444445, 0.6513888888888889, 0.71, 0.7686111111111111, 0.8247222222222222, 0.8838888888888888, 0.9436111111111111, 1.0033333333333334, 1.0641666666666667, 1.1252777777777778, 1.1869444444444444, 1.248888888888889, 1.3102777777777779, 1.3730555555555555, 1.4358333333333333, 1.4961111111111112, 1.5577777777777777, 1.62, 1.6833333333333333, 1.7455555555555555, 1.8038888888888889, 1.8669444444444445, 1.9288888888888889, 1.9894444444444443, 2.0522222222222224, 2.1133333333333333, 2.1758333333333333, 2.238611111111111, 2.301111111111111, 2.3630555555555555, 2.424722222222222, 2.486111111111111, 2.549166666666667, 2.611666666666667, 2.6752777777777776, 2.738611111111111, 2.8002777777777776, 2.862222222222222, 2.924722222222222, 2.9869444444444446, 3.05, 3.111388888888889, 3.1752777777777776, 3.2383333333333333, 3.2994444444444446, 3.3608333333333333, 3.423611111111111, 3.485, 3.548611111111111, 3.6033333333333335, 3.6555555555555554, 3.6997222222222224, 3.7441666666666666, 3.7880555555555557, 3.8325, 3.8769444444444443, 3.921111111111111, 3.9644444444444447, 4.013333333333334, 4.0575, 4.101111111111111, 4.147777777777778, 4.191944444444444, 4.238611111111111, 4.286388888888889, 4.335555555555556, 4.385, 4.431111111111111, 4.475833333333333, 4.5225, 4.570277777777778, 4.618888888888889, 4.666388888888889, 4.7125, 4.761388888888889, 4.809444444444445, 4.857777777777778, 4.904444444444445, 4.953055555555555, 5.000555555555556, 5.048055555555556, 5.1075, 5.167777777777777, 5.229166666666667, 5.288611111111111, 5.345277777777778, 5.406666666666666, 5.4686111111111115, 5.528611111111111, 5.588888888888889, 5.648888888888889, 5.709722222222222, 5.770555555555555, 5.8308333333333335, 5.891388888888889, 5.952222222222222, 6.0125, 6.073611111111111, 6.135833333333333, 6.1963888888888885, 6.256388888888889, 6.3436111111111115, 6.453055555555555, 6.545277777777778, 6.605555555555555, 6.665277777777778, 6.724166666666667, 6.781666666666666, 6.842222222222222, 6.901111111111111, 6.96, 7.020555555555555, 7.081388888888889, 7.1433333333333335, 7.205, 7.266111111111111, 7.326388888888889, 7.3875, 7.447777777777778, 7.5088888888888885, 7.569444444444445, 7.629722222222222, 7.690833333333333, 7.751111111111111, 7.812222222222222, 7.873055555555555, 7.934722222222222, 7.995833333333334, 8.05638888888889, 8.116388888888888, 8.177777777777777, 8.239166666666666, 8.300277777777778, 8.36111111111111, 8.421666666666667, 8.482777777777779, 8.544444444444444, 8.605833333333333, 8.6675, 8.72861111111111, 8.794722222222223, 8.855277777777777, 8.91638888888889, 8.978055555555555, 9.040555555555555, 9.101666666666667, 9.163611111111111, 9.224444444444444, 9.285, 9.345555555555556, 9.40611111111111, 9.468055555555555, 9.526388888888889, 9.587222222222222, 9.650833333333333, 9.713333333333333, 9.775, 9.835277777777778, 9.926944444444445, 9.987777777777778, 10.049722222222222, 10.111666666666666, 10.17388888888889, 10.236666666666666, 10.299166666666666, 10.358888888888888, 10.42, 10.480833333333333, 10.543055555555556, 10.605277777777777, 10.6675, 10.727777777777778, 10.815, 10.946388888888889, 11.090277777777779, 11.228333333333333, 11.346944444444444, 11.40611111111111, 11.465, 11.522777777777778, 11.571944444444444, 11.619444444444444, 11.664722222222222, 11.708333333333334, 11.75611111111111, 11.803888888888888, 11.85361111111111, 11.900277777777777, 11.948333333333334, 11.9975, 12.0475, 12.095277777777778, 12.144722222222223, 12.189722222222223, 12.238055555555556, 12.286111111111111, 12.335833333333333, 12.383888888888889, 12.431666666666667, 12.47861111111111, 12.528888888888888, 12.576944444444445, 12.625277777777777, 12.6725, 12.722222222222221, 12.769444444444444, 12.818888888888889, 12.865555555555556, 12.9125, 12.962222222222222, 13.010555555555555, 13.05888888888889, 13.105555555555556, 13.154444444444444, 13.202222222222222, 13.251666666666667, 13.296666666666667, 13.345, 13.391944444444444, 13.442777777777778, 13.490833333333333, 13.539166666666667, 13.586666666666666, 13.637222222222222, 13.685555555555556, 13.733333333333333, 13.78, 13.829444444444444, 13.8775, 13.925833333333333, 13.9725, 14.020833333333334, 14.069444444444445, 14.11888888888889, 14.16611111111111, 14.213055555555556, 14.261944444444444, 14.310555555555556, 14.359166666666667, 14.40611111111111, 14.453888888888889],
            'y': [22.2, 32.800000000000004, 45.5, 50.9, 63.800000000000004, 67.60000000000001, 68.10000000000001, 72.6, 70.89999999999999, 76.9, 78.2, 79.4, 78.7, 81.6, 81.89999999999999, 83.89999999999999, 85.2, 76.3, 87.4, 85.5, 84.7, 84.89999999999999, 85.9, 86.8, 89.1, 87.4, 89.2, 87.9, 86.1, 89.0, 85.6, 88.9, 87.6, 88.5, 86.5, 90.2, 88.7, 90.10000000000001, 89.3, 91.7, 83.8, 92.2, 91.3, 90.10000000000001, 90.5, 92.0, 89.3, 92.10000000000001, 93.60000000000001, 85.2, 92.2, 92.10000000000001, 91.9, 90.3, 93.8, 92.30000000000001, 91.5, 94.19999999999999, 93.10000000000001, 87.1, 92.9, 92.30000000000001, 93.89999999999999, 91.8, 94.39999999999999, 92.30000000000001, 93.10000000000001, 90.8, 93.2, 93.60000000000001, 93.5, 93.2, 93.60000000000001, 92.7, 92.9, 90.9, 93.89999999999999, 90.10000000000001, 92.60000000000001, 93.5, 94.1, 93.2, 94.1, 90.10000000000001, 93.30000000000001, 93.0, 94.0, 91.10000000000001, 92.2, 93.2, 94.6, 92.7, 92.60000000000001, 93.60000000000001, 94.39999999999999, 94.6, 92.80000000000001, 93.0, 93.2, 93.4, 87.8, 94.8, 93.4, 93.0, 93.89999999999999, 93.60000000000001, 94.8, 91.0, 90.60000000000001, 94.89999999999999, 94.3, 94.3, 94.8, 92.7, 91.60000000000001, 93.0, 93.8, 93.7, 93.7, 91.3, 94.3, 94.39999999999999, 85.3, 93.4, 93.8, 94.6, 92.10000000000001, 94.19999999999999, 95.19999999999999, 93.2, 92.80000000000001, 94.19999999999999, 93.5, 93.2, 94.3, 94.69999999999999, 94.0, 93.4, 94.3, 94.69999999999999, 94.6, 93.5, 89.4, 92.5, 94.3, 92.60000000000001, 95.19999999999999, 94.5, 92.0, 93.7, 92.0, 94.8, 94.19999999999999, 95.8, 89.5, 93.0, 91.60000000000001, 93.8, 94.6, 93.2, 92.4, 94.8, 94.89999999999999, 92.5, 95.0, 92.10000000000001, 95.6, 93.60000000000001, 94.69999999999999, 94.8, 92.9, 90.8, 95.19999999999999, 93.30000000000001, 94.8, 93.2, 93.89999999999999, 94.6, 94.1, 94.19999999999999, 92.7, 95.5, 95.5, 91.2, 95.39999999999999, 93.2, 93.4, 93.5, 93.60000000000001, 94.39999999999999, 92.2, 94.89999999999999, 95.39999999999999, 91.7, 92.2, 93.4, 95.0, 92.9, 95.1, 93.30000000000001, 92.5, 94.89999999999999, 92.60000000000001, 95.3, 94.8, 95.1, 94.1, 90.0, 94.89999999999999, 87.3, 87.3, 90.9, 87.6, 94.3, 95.1, 94.6, 95.3, 93.7, 95.0, 94.3, 93.5, 93.60000000000001, 94.8, 94.8, 90.8, 95.89999999999999, 94.8, 37.2, 95.8, 91.60000000000001, 67.80000000000001, 92.9, 94.89999999999999, 94.8, 94.3, 94.1, 95.7, 84.2, 94.39999999999999, 94.0, 91.7, 94.19999999999999, 95.0, 93.60000000000001, 94.3, 95.19999999999999, 93.60000000000001, 95.19999999999999, 93.60000000000001, 94.89999999999999, 94.6]
        },
    };

    // Helper to create trace
    const createTrace = (name, color, x, y) => ({
        x: x,
        y: y,
        type: 'scatter',
        mode: 'lines+markers',
        name: name,
        line: { color: color, width: 2 },
        marker: { size: 4 },

        hovertemplate: '%{fullData.name}: %{y:.2f}%<extra></extra>'
    });

    const plotData = [
        createTrace('SSG', '#ffb6c1', timetakenData['SSG']['x'], timetakenData['SSG']['y']),      // Light Pink
        createTrace('MSG', '#90ee90', timetakenData['MSG']['x'], timetakenData['MSG']['y']),      // Light Green
        createTrace('MRG', '#add8e6', timetakenData['MRG']['x'], timetakenData['MRG']['y']),      // Light Blue
        createTrace('SSG+DP', '#8b0000', timetakenData['SSG+DP']['x'], timetakenData['SSG+DP']['y']), // Dark Red
        createTrace('MSG+DP', '#006400', timetakenData['MSG+DP']['x'], timetakenData['MSG+DP']['y']), // Dark Green
        createTrace('MRG+DP', '#00008b', timetakenData['MRG+DP']['x'], timetakenData['MRG+DP']['y']), // Dark Blue
    ];

    return (
        <div className="robustness-container">
            <Plot
                data={plotData}
                layout={{
                    autosize: true,
                    title: 'Time Taken to Train vs. Test Accuracy',
                    paper_bgcolor: 'rgba(0,0,0,0)',
                    plot_bgcolor: 'rgba(0,0,0,0)',
                    font: { color: 'lightgray' },
                    xaxis: { 
                        title: {
                            text: "Time Taken (h)",
                            standoff: 10
                        },
                        gridcolor: 'rgba(255,255,255,0.1)',
                        unifiedhovertitle: { text: 'Training Time: %{x:.2f} h' }
                    },
                    yaxis: {
                        title: {
                            text: 'Test Accuracy (%)',
                            standoff: 10
                        },
                        range: [0, 100],
                        tickformat: '.0f',
                        ticksuffix: '%',
                        gridcolor: 'rgba(255,255,255,0.1)'
                    },
                    legend: {
                        orientation: 'h',
                        x: 0.5,
                        xanchor: 'center',
                        xref: 'paper',
                        yref: 'paper',
                        bgcolor: 'rgba(0,0,0,0)'
                    },
                    hoverlabel: {
                        bgcolor: 'rgba(0,0,0,0)',     // transparent
                        bordercolor: 'rgba(0,0,0,0)', // no border
                        font: { color: 'lightgray' },
                    },
                    margin: { l: 70, r: 20, b: 50, t: 50, pad: 4 },
                    hovermode: 'x unified' // Shows all values at a specific x-point on hover
                }}
                useResizeHandler={true}
                style={{ width: '100%', height: '100%' }}
                config={{ displayModeBar: false }}
            />
        </div>
    );
}


const BlogIndex = 0;

// For Table of Contents
const tocItems = [
    ['1 Prerequisites', '1.1 Project Folder Organization Overview', '1.2 Setting Up Dependencies', '1.3 Getting the Dataset'],  // First section
    ['2 Data Preprocessing', '2.1 Converting STL to PCD', '2.2 Preprocessing PCD (Optional)'],  // Second section
    ['3 Data Loading and Preparation'],  // Third section
    ['4 Training and Evaluating the Model', '4.1 Training Loop', '4.2 Evaluation Loop', '4.3 Model Checkpointing and Log Management'],  // Fourth section
    ['5 Model Evaluation', '5.1 Robustness to Non-Uniform Point Density', '5.2 t-SNE Feature Embedding', '5.3 Confusion Matrix', '5.4 Training Time Analysis'],  // Fifth Section
    ['6 PCD Visualizer', '6.1 Preprocessing Visualizer', '6.2 Data Augmentation Visualizer'],  // Sixth Section
];

const titleContent = blogpage_title_content({ BlogIndex: BlogIndex, tocItems: tocItems });

const sectionLinks = [...titleContent.sectionLinks];
const tocTitles = [...titleContent.tocTitles];

// For References
const ReferencesItems = [
    {
        citationText: "Qi, C.R., Yi, L., Su, H. and Guibas, L.J., 2017. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30",
        href: "https://arxiv.org/abs/1706.02413",
    },
    {
        citationText: "Yuksel, C., 2015, May. Sample elimination for generating poisson disk sample sets. In Computer Graphics Forum (Vol. 34, No. 2, pp. 25-32)",
        href: "https://core.ac.uk/download/pdf/204733579.pdf",
    }
];

const referencesContent = blogpage_references_content({ ReferencesItems: ReferencesItems });

const BlogPage2Data = [
    // Title Section
    titleContent,

    // Content Section
    {
        type: 'div', className: 'BlogPage_ContentSection', data: [
            // Summary Section
            { type: 'span', className: 'section_title sectionIcon', id: 'summary', icon: faSection, data: 'Summary' },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'In my ' },
                    { type: 'Link', to: "https://blank-ed.github.io/ilyas_dawoodjee/#/blogpage/understanding_a_intuitive_visual", className: 'blog_links', data: "previous blog post" },
                    { type: 'Text', data: ', I conducted an in-depth exploration of PointNet++ ' },
                    { type: 'Link', to: '#ref_1', className: 'blog_links', data: [{ type: 'Text', data: '[1]' }] },
                    { type: 'Text', data: '. I broke down its core principles into intuitive, visual explanations that guided readers (from novice to professionals) through each step of the process. In this part 2 of the blog post, I will guide you through on training a custom PointNet++ model using a ' },
                    { type: 'Link', to: "https://github.com/madlabub/Machining-feature-dataset", className: 'blog_links', data: "machining feature dataset" },
                    { type: 'Text', data: '. For further details on the dataset please ' },
                    { type: 'Link', to: "https://blank-ed.github.io/ilyas_dawoodjee/#/blogpage/understanding_a_intuitive_visual#sixth", className: 'blog_links', data: "refer to the relevant section" },
                    { type: 'Text', data: ' of the previous blog post.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'My goal is to recreate Figure 4 in Section 4 of the PointNet++ ' },
                    { type: 'Link', to: '#ref_1', className: 'blog_links', data: [{ type: 'Text', data: '[1]' }] },
                    { type: 'Text', data: ' paper where it illustrates the effectiveness of the density adaptive strategy in handling non-uniform point cloud density. For my case, I will train and compare six variants of PointNet++ models for classification with and without Random Input Dropout (DP): Single-Scale Grouping (SSG), SSG+DP, Multi-Scale Grouping (MSG), MSG+DP, Multi-Resolution Grouping (MRG), and MRG+DP. ' },
                    { type: 'Link', to: "#fig_1", className: 'blog_links', data: "Figure 1" },
                    { type: 'Text', data: ' shows a Point Cloud Data (PCD) with different levels of DP points:' },
                ]
            },
            { type: 'img', id: 'fig_1', className: 'Blog_Image', data: [Figure1, 'Figure 1: Varying Levels of Random Input Dropout', ''] },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'All the models are trained and evaluated initially on ' },
                    { type: 'span', className: 'code', data: '1024' },
                    { type: 'Text', data: ' PCD points but then are interfaced with testing data with varying non-uniform point cloud density as shown in ' },
                    { type: 'Link', to: "#fig_1", className: 'blog_links', data: "Figure 1" },
                    { type: 'Text', data: '. Throughout this blog, I will cover important steps ranging from preprocessing data to model evaluation intuitively using visuals.' },
                ]
            },

            // First Section - Prerequisites
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To get started, download the ' },
                    { type: 'Link', to: "https://github.com/blank-ed/PointNet2_MFD", className: 'blog_links', data: "GitHub repository" },
                    { type: 'Text', data: ' by running the shell command below. This contains everything you need to train a custom PointNet++ classification model for the machining feature dataset. It also includes essential directories and files necessary for initial setup and operation.' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "git clone https://github.com/blank-ed/PointNet2_MFD.git" }],
                ]
            },

            // First-First Section - Project Folder Organization Overview
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            { type: 'span', className: 'body', data: [ { type: 'Text', data: 'The project folder is organized as follows:' }, ] },
            {
                type: 'CodeBlock', title: 'PlainText', extraText: [
                    [{ type: 'plaintext', code: "PointNet2_MFD/" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " Blog_Logs/                     # Pre-trained logs and models used for this blog post results" }],
                    [{ type: 'plaintext', code: "    MRG_Dropout/               # Trained MRG model with dropout" }],
                    [{ type: 'plaintext', code: "    MRG_NoDropout/             # Trained MRG model with no dropout" }],
                    [{ type: 'plaintext', code: "    MSG_Dropout/               # Trained MSG model with dropout" }],
                    [{ type: 'plaintext', code: "    MSG_NoDropout/             # Trained MSG model with no dropout" }],
                    [{ type: 'plaintext', code: "    SSG_Dropout/               # Trained SSG model with dropout" }],
                    [{ type: 'plaintext', code: "    SSG_NoDropout/             # Trained SSG model with no dropout" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " Logs/                          # Storing log files (subfolders created at runtime)" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " MFD_dataset/                   # Dataset files" }],
                    [{ type: 'plaintext', code: "    raw/                       # Unprocessed STL files" }],
                    [{ type: 'plaintext', code: "    pcd/                       # Raw point clouds from STL (subfolder created at runtime)" }],
                    [{ type: 'plaintext', code: "    preprocessed_pcd_x/        # Cleaned/processed data of user specified x points (subfolder created at runtime)" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " models/                        # PointNet++ models" }],
                    [{ type: 'plaintext', code: "    __init__.py                # Makes 'models' a package and exposes model classes for easy importing" }],
                    [{ type: 'plaintext', code: "    PointNet2_CLS_MRG.py       # Multi-resolution grouping classification model" }],
                    [{ type: 'plaintext', code: "    PointNet2_CLS_MSG.py       # Multi-scale grouping classification model" }],
                    [{ type: 'plaintext', code: "    PointNet2_CLS_SSG.py       # Single-scale grouping classification model" }],
                    [{ type: 'plaintext', code: "    PointNet2_Loss.py          # PointNet+ loss function for classification" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " utils/                         # Utilities folder" }],
                    [{ type: 'plaintext', code: "    __init__.py                # Makes 'utils' a package and aggregates key utility functions" }],
                    [{ type: 'plaintext', code: "    PointNet2_AugmentData.py   # Data augmentation functions" }],
                    [{ type: 'plaintext', code: "    PointNet2_Utils.py         # Required utilities for models" }],
                    [{ type: 'plaintext', code: "    PointNet2DataLoader.py     # To load train and test data" }],
                    [{ type: 'plaintext', code: "    utils.py                   # Required utilities for preprocessing data, training, and testing model" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " visualizers/                   # Scripts for generating figures and interactive plots" }],
                    [{ type: 'plaintext', code: "    __init__.py                # Makes 'visualizers' a package and exposes visualization helper modules" }],
                    [{ type: 'plaintext', code: "    Augmentation.py            # Visualizes data augmentation effects" }],
                    [{ type: 'plaintext', code: "    BallQuery_vs_kNN.py        # Visualizes difference between Ball Query and k-NN grouping" }],
                    [{ type: 'plaintext', code: "    ConfusionMatrix_Plot.py    # Generates plot for visualizing confusion matrix" }],
                    [{ type: 'plaintext', code: "    FPS_vs_RandomSampling.py   # Visualizes difference between Farthest Point Sampling and Random Sampling" }],
                    [{ type: 'plaintext', code: "    PreprocessingPCD.py        # Visualizes PCD data preprocessing" }],
                    [{ type: 'plaintext', code: "    Robustness_Plot.py         # Generates graph to visualize robustness to non-uniform point densities" }],
                    [{ type: 'plaintext', code: "    TimeTaken_Plot.py          # Generates graph to visualize how long it takes to train each model" }],
                    [{ type: 'plaintext', code: "    TSNE_Plot.py               # Generates t-SNE plots to visualize latent feature clusters" }],
                    [{ type: 'plaintext', code: "    VisualizePCD_Class.py      # Visualizer class for visualizers above (except for ConfusionMatrix_Plot.py, Robustness_Plot.py, TimeTaken_Plot.py, and TSNE_Plot.py)" }],
                    [{ type: 'plaintext', code: "" }],
                    [{ type: 'plaintext', code: " README.md                      # Python overview and instructions" }],
                    [{ type: 'plaintext', code: " requirements.txt               # Python dependencies" }],
                    [{ type: 'plaintext', code: " test_classification.py         # Code to test classification model" }],
                    [{ type: 'plaintext', code: " train_classification.py        # Code to train classification model" }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The core of the code revolves around the ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset' },
                    { type: 'Text', data: ' directory, which contains three subfolders: ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/raw' },
                    { type: 'Text', data: ', ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/pcd' },
                    { type: 'Text', data: ', ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/preprocessed_pcd_x' },
                    { type: 'Text', data: ' where ' },
                    { type: 'span', className: 'code', data: 'x' },
                    { type: 'Text', data: ' is the user specified number of points used for training the models. It is recommended to download and extract the unprocessed STL machining feature dataset into the ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/raw' },
                    { type: 'Text', data: ' folder. You can refer to ' },
                    { type: 'Link', to: "#first_third", className: 'blog_links', data: "this section" },
                    { type: 'Text', data: ' for more details. The raw STL files are then converted into PCD, which is stored in the ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/pcd' },
                    { type: 'Text', data: ' folder. After preprocessing and cleaning, the processed PCD is saved in the ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/preprocessed_pcd_x' },
                    { type: 'Text', data: ' folder.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The project comprises of several Python scripts that handles different aspects of the workflow:' },
                ]
            },
            {
                type: 'ul', className: 'body', data: [
                    { type: 'li', data: [{ type: 'strong', data: 'Model Implementations: ' }, {type: 'Text', data: 'The '}, {type: 'span', className: 'code', data: 'models/'}, { type: 'Text', data: ' folder contains scripts such as ' }, { type: 'span', className: 'code', data: 'PointNet2_CLS_MRG.py' }, { type: 'Text', data: ', ' },{ type: 'span', className: 'code', data: 'PointNet2_CLS_MSG.py' }, { type: 'Text', data: ', and ' },{ type: 'span', className: 'code', data: 'PointNet2_CLS_SSG.py' }, { type: 'Text', data: ' that defines classification models based on the PointNet++ architecture, utilizing different grouping strategies. Please refer to ' }, { type: 'Link', to: "https://blank-ed.github.io/ilyas_dawoodjee/#/blogpage/understanding_a_intuitive_visual#third_fifth", className: 'blog_links', data: "this section" }, { type: 'Text', data: ' of the previous blog for more details.' }] },
                    { type: 'li', data: [{ type: 'strong', data: 'Utility & Visualization Scripts: ' }, { type: 'Text', data: 'The ' }, { type: 'span', className: 'code', data: 'utils/' }, { type: 'Text', data: ' folder houses essential tools: ' }, { type: 'span', className: 'code', data: 'PointNet2_Utils.py' }, { type: 'Text', data: ' contains functions such as ' }, { type: 'span', className: 'code', data: 'PointNetSetAbstraction' }, { type: 'Text', data: ' and ' }, { type: 'span', className: 'code', data: 'PointNetSetAbstractionMsg' }, { type: 'Text', data: ' layers, ' }, { type: 'span', className: 'code', data: 'PointNet2_AugmentData.py' }, { type: 'Text', data: ' contains an ' }, { type: 'span', className: 'code', data: 'AugmentData' }, { type: 'Text', data: ' class for data augmentation, while ' }, { type: 'span', className: 'code', data: 'utils.py' }, { type: 'Text', data: ' contains common parser function (used in '}, { type: 'span', className: 'code', data: 'train_classification.py' }, { type: 'Text', data: ', '}, { type: 'span', className: 'code', data: 'test_classification.py' }, { type: 'Text', data: ', '}, { type: 'span', className: 'code', data: 'visualizers/TSNE_Plot.py' }, { type: 'Text', data: ', and '}, { type: 'span', className: 'code', data: 'visualizers/ConfusionMatrix_Plot.py' }, { type: 'Text', data: '), evaluation metrics, preprocessing tools (such as converting STL files to PCD, and preprocessing PCD), and functions to setup and handle loggers used to log data. Additionally, the ' }, { type: 'span', className: 'code', data: 'visualizers/' }, { type: 'Text', data: ' directory provides a suite of visualization scripts for a better understanding of algorithms or visualizing data such as generating figures and interactive plots, such as ' }, { type: 'span', className: 'code', data: 'Augmentation.py' }, { type: 'Text', data: ', ' }, { type: 'span', className: 'code', data: 'BallQuery_vs_kNN.py' }, { type: 'Text', data: ', ' }, { type: 'span', className: 'code', data: 'FPS_vs_RandomSampling.py' }, { type: 'Text', data: ', and ' }, { type: 'span', className: 'code', data: 'PreprocessingPCD.py' }, { type: 'Text', data: ' all utilizing the helper class found in ' }, { type: 'span', className: 'code', data: 'VisualizePCD_Class.py' }, { type: 'Text', data: '. Moreover, there are scripts dedicated to visualizing the results of the trained models, such as ' }, { type: 'span', className: 'code', data: 'TSNE_Plot.py' }, {type: 'Text', data: ', '}, { type: 'span', className: 'code', data: 'Robustness_Plot.py' }, {type: 'Text', data: ', '}, { type: 'span', className: 'code', data: 'ConfusionMatrix_Plot.py' }, {type: 'Text', data: ', and '}, { type: 'span', className: 'code', data: 'TimeTaken_Plot.py' }, { type: 'Text', data: '. Lastly, ' }, { type: 'span', className: 'code', data: 'PointNet2DataLoader.py' }, { type: 'Text', data: ' manages the loading and preprocessing (if necessary) of training and testing data.' }] },
                    { type: 'li', data: [{ type: 'strong', data: 'Training, Testing, and Data Handling: ' }, { type: 'Text', data: 'The ' }, { type: 'span', className: 'code', data: 'train_classification.py' }, { type: 'Text', data: ' and ' }, { type: 'span', className: 'code', data: 'test_classification.py' }, { type: 'Text', data: ' handles the training and evaluation of the classification models.' }] },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'Additionally, the ' },
                    { type: 'span', className: 'code', data: 'Logs/' },
                    { type: 'Text', data: ' folder is used to store your runtime checkpoints and training logs, while the '},
                    { type: 'span', className: 'code', data: 'Blog_Logs/' },
                    { type: 'Text', data: " folder contains the pre-trained models and logs used to generate the results presented in this blog post." },
                ]
            },

            // First-Second Section - Setting Up Dependencies
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'Before we start training the model, please note that I am using Python 3.12. There are a few libraries we need to install, and you can install all of them directly by running the shell script below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "cd PointNet2_MFD" }],
                    [{ type: 'shell', code: 'python -m venv .venv'}],
                    [{ type: 'shell', code: '.venv\\Scripts\\activate'}],
                    [{ type: 'shell', code: "pip install -r requirements.txt" }],
                ]
            },
            { 
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'This will create a virtual environment and install the latest versions of the required libraries for the most part, except for ' },
                    { type: 'span', className: 'code', data: 'torch' },
                    { type: 'Text', data: ', ' },
                    { type: 'span', className: 'code', data: 'torchvision' },
                    { type: 'Text', data: ', and ' },
                    { type: 'span', className: 'code', data: 'torchaudio' },
                    { type: 'Text', data: ' which may require specific installation instructions depending on your system configuration (e.g., OS, Python version, and CUDA version). To ensure optimal compatibility and performance, please refer to the '},
                    { type: 'Link', to: "https://pytorch.org/", className: 'blog_links', data: "official PyTorch installation guide" },
                    { type: 'Text', data: ' to select the best installation method for your environment. For the rest of the libraries, if you want to stick with the versions that I used, you can run the shell script below:' },
                ] 
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "pip install fpsample==0.3.3 tqdm==4.67.1 scikit-learn==1.6.1 open3d==0.19.0 prettytable==3.12.0 PySide6==6.8.2.1 plotly==6.5.0" }],
                ]
            },

            // First-Third Section - Getting the Dataset
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            { 
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To download the dataset, you can '},
                    { type: 'span', className: 'code', data: 'git clone' },
                    { type: 'Text', data: ' from this '},
                    { type: 'Link', to: "https://github.com/madlabub/Machining-feature-dataset.git", className: 'blog_links', data: "repository" },
                    { type: 'Text', data: ', extract the '},
                    { type: 'span', className: 'code', data: 'dataset.rar' },
                    { type: 'Text', data: ' and copy all 24 folders from inside '},
                    { type: 'span', className: 'code', data: 'dataset/stl' },
                    { type: 'Text', data: ' folder into the '},
                    { type: 'span', className: 'code', data: 'MFD_dataset/raw' },
                    { type: 'Text', data: ' folder.' },
                ] 
            },
            { type: 'Collapsable', colType: 'warning', title: 'Warning', extraText: "Please be aware that 505 files in this dataset have naming discrepancies. However, this won't cause any issues during training as you can simply use the label from the folder name instead of relying on each individual files's name." },
            

            // Second Section - Data Preprocessing
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'This section explains the necessary steps taken for preprocessing the raw STL files by converting it into PCD with an option to preprocess PCD, after which they are used to train a custom PointNet++ classification model.' },
                ]
            },

            // Second-First Section - Converting STL to PCD
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "The function for converting STL to PCD, " },
                    { type: 'span', className: 'code', data: 'stl_to_pcd' },
                    { type: 'Text', data: ", is given in " },
                    { type: 'span', className: 'code', data: 'utils/utils.py' },
                    { type: 'Text', data: " script:" },
                ]
            },
            {
                type: 'CodeBlock', title: 'Python', extraText: [
                    [{ type: 'e', code: '# Import global modules' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' numpy ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' np' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' pathlib ' }, { type: 'a', code: 'import' }, { type: 'b', code: ' Path' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' tqdm ' }, { type: 'a', code: 'import' }, { type: 'b', code: ' tqdm' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' open3d ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' o3d' }],
                    [{ type: '', code: '' }],

                    // Function Definition
                    [{ type: 'a', code: 'def' }, { type: 'g', code: ' stl_to_pcd' }, { type: 'b', code: '(root: ' }, { type: 'l', code: 'Path' }, { type: 'b', code: ', num_points: ' }, { type: 'l', code: 'int' }, { type: 'b', code: ' = ' }, { type: 'h', code: '10000' }, { type: 'b', code: ', use_normals: ' }, { type: 'l', code: 'bool' }, { type: 'b', code: ' = ' }, { type: 'a', code: 'True' }, { type: 'b', code: ') -> ' }, { type: 'a', code: 'None' }, { type: 'b', code: ':' }],

                    // Docstring
                    [{ type: 'd', code: '    """' }],
                    [{ type: 'd', code: '    Converts STL files to point clouds and optionally includes normals.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :param root: Path of the PointNet2_MFD folder' }],
                    [{ type: 'd', code: '    :param num_points: Number of points to sample from each STL file.' }],
                    [{ type: 'd', code: '    :param use_normals: Whether to include surface normals in the output files. If True, the output files will contain' }],
                    [{ type: 'd', code: '    both point coordinates (x, y, z) and normal vectors (nx, ny, nz).' }],
                    [{ type: 'd', code: '    :return: None' }],
                    [{ type: 'd', code: '    """' }],

                    // Body - Path Setup
                    [{ type: 'b', code: '    stl_data = root / ' }, { type: 'c', code: "'MFD_dataset/raw'" }, { type: 'e', code: '  # Path to the raw stl files' }],
                    [{ type: '', code: '' }],

                    [{ type: 'b', code: '    pcd = root / ' }, { type: 'c', code: "'MFD_dataset/pcd'" }],
                    [{ type: 'b', code: '    pcd.mkdir(' }, { type: 'f', code: 'parents' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ', ' }, { type: 'f', code: 'exist_ok' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ')' }, { type: 'e', code: '  # create a folder to store point cloud data' }],
                    [{ type: '', code: '' }],

                    // Outer Loop
                    [{ type: 'a', code: '    for' }, { type: 'b', code: ' folders ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' tqdm(stl_data.iterdir(), ' }, { type: 'f', code: 'desc' }, { type: 'b', code: '=' }, { type: 'c', code: "f'Converting STL to PCD with " }, { type: 'a', code: "{"}, { type: 'b', code: 'num_points'}, { type: 'a', code: "}"}, { type: 'c', code: " points'"}, { type: 'b', code: ', ' }, { type: 'f', code: 'total' }, { type: 'b', code: '=' }, { type: 'l', code: 'len'}, { type: 'b', code: '(os.listdir(stl_data))):' }],
                    [{ type: 'b', code: '        subfolder_path = pcd / folders.stem' }, { type: 'e', code: '  # Create subdirectory for the folder in pcd' }],
                    
                    [{ type: 'b', code: '        subfolder_path.mkdir(' }, { type: 'f', code: 'parents' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ', ' }, { type: 'f', code: 'exist_ok' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ')' }, { type: 'e', code: '  # Create the subfolder if it doesn\'t exist' }],
                    [{ type: '', code: '' }],

                    // Inner Loop
                    [{ type: 'a', code: '        for' }, { type: 'b', code: ' files ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' folders.iterdir():' }],
                    [{ type: 'b', code: '            save_path = subfolder_path / ' }, { type: 'c', code: 'f"{' }, { type: 'b', code: 'files.stem' }, { type: 'c', code: '}.txt"' }],
                    [{ type: '', code: '' }],

                    [{ type: 'e', code: '            # Skip processing if the output file already exists' }],
                    [{ type: 'a', code: '            if' }, { type: 'b', code: ' save_path.exists():' }],
                    [{ type: 'a', code: '                continue' }],
                    [{ type: '', code: '' }],

                    [{ type: 'b', code: '            mesh = o3d.io.read_triangle_mesh(' }, { type: 'f', code: 'filename' }, { type: 'b', code: '=files)' }, { type: 'e', code: '  # Read STL and sample points from the mesh' }],
                    
                    [{ type: 'b', code: '            pcd = mesh.sample_points_poisson_disk(num_points)' }, { type: 'e', code: '  # Open3D PointCloud object with 10,000 points' }],
                    
                    [{ type: 'b', code: '            pcd_array = np.asarray(pcd.points)' }, { type: 'e', code: '  # Store the points as an array' }],
                    [{ type: '', code: '' }],

                    // Use Normals Logic
                    [{ type: 'a', code: '            if' }, { type: 'b', code: ' use_normals:' }],
                    [{ type: 'e', code: '                # Estimate normals for the point cloud within a specified radius or up to a maximum number of nearest neighbors (max_nn) for each point' }],
                    [{ type: 'b', code: '                pcd.estimate_normals(' }, { type: 'f', code: 'search_param' }, { type: 'b', code: '=o3d.geometry.KDTreeSearchParamHybrid(' }, { type: 'f', code: 'radius' }, { type: 'b', code: '=' }, { type: 'h', code: '1' }, { type: 'b', code: ', ' }, { type: 'f', code: 'max_nn' }, { type: 'b', code: '=' }, { type: 'h', code: '30' }, { type: 'b', code: '))' }],
                    
                    [{ type: 'b', code: '                pcd.orient_normals_consistent_tangent_plane('}, { type: 'f', code: 'k'}, { type: 'b', code: '=' }, { type: 'h', code: '15' }, { type: 'b', code: ')' }, { type: 'e', code: '  # Orient normals consistently' }],
                    
                    [{ type: 'b', code: '                normals_array = np.asarray(pcd.normals)' }, { type: 'e', code: '  # Convert to numpy array for stacking purposes' }],
                    [{ type: '', code: '' }],
                    
                    [{ type: 'e', code: '                # Combine points and normals into Nx6 array' }],
                    [{ type: 'b', code: '                data_array = np.hstack((pcd_array, normals_array))' }],
                    
                    [{ type: 'a', code: '            else' }, { type: 'b', code: ':' }],
                    [{ type: 'e', code: '                # Use points only, Nx3 array' }],
                    [{ type: 'b', code: '                data_array = pcd_array' }],
                    [{ type: '', code: '' }],

                    [{ type: 'b', code: '            np.savetxt(save_path, data_array, ' }, { type: 'f', code: 'delimiter' }, { type: 'b', code: '=' }, { type: 'c', code: '","' }, { type: 'b', code: ', ' }, { type: 'f', code: 'fmt' }, { type: 'b', code: '=' }, { type: 'c', code: '"%.6f"' }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    // Print Statement
                    [{ type: 'l', code: '    print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Converted STL files to point cloud with {" }, { type: 'b', code: 'num_points' }, { type: 'c', code: "}'" }, { type: 'b', code: ')' }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "This function goes through each of the machining features in the " },
                    { type: 'span', className: 'code', data: 'MFD_dataset/raw' },
                    { type: 'Text', data: " folder and converts every STL file to PCD of a user defined number of points (default is set to " },
                    { type: 'span', className: 'code', data: '10,000' },
                    { type: 'Text', data: ") with an option to include normals (default is set to " },
                    { type: 'span', className: 'code', data: 'True' },
                    { type: 'Text', data: "). This is done by using " },
                    { type: 'span', className: 'code', data: 'open3d' },
                    { type: 'Text', data: " library which is an open-source library designed for processing, visualizing, and analyzing 3D data. Since our dataset consists of STL format CAD models, which means that it uses triangles to describe the surface of a 3D model, the function in " },
                    { type: 'span', className: 'code', data: 'open3d' },
                    { type: 'Text', data: " that helps read this type of file format is called " },
                    { type: 'span', className: 'code', data: 'read_triangle_mesh' },
                    { type: 'Text', data: "." },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "It then returns to a mesh object, consisting of a structured data representation of the triangle mesh, including its vertices, edges, and faces, for further processing and visualization. This mesh is then used to obtain the PCD of user defined number of points using " },
                    { type: 'span', className: 'code', data: 'sample_points_poisson_disk' },
                    { type: 'Text', data: " function. Based on " },
                    { type: 'Link', to: '#ref_2', className: 'blog_links', data: [{ type: 'Text', data: '[2]' }] },
                    { type: 'Text', data: ", this function generates a uniformly distributed point cloud by applying Poisson disk sampling to a mesh, a technique that randomly distributes points while ensuring each maintains a minimum distance from its neighbors to prevent clustering." },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "If the user decides to include normals of the PCD, then the function " },
                    { type: 'span', className: 'code', data: 'estimate_normals' },
                    { type: 'Text', data: " from " },
                    { type: 'span', className: 'code', data: 'open3d' },
                    { type: 'Text', data: " is used to estimate the normals using " },
                    { type: 'span', className: 'code', data: 'KDTreeSearchParamHybrid(radius=r, max_nn=n)' },
                    { type: 'Text', data: " search parameter, where it considers points within a user specified radius, " },
                    { type: 'span', className: 'code', data: 'r' },
                    { type: 'Text', data: ", but only up to user specified n nearest neighbors. Alternatively, you can use other search parameters like " },
                    { type: 'span', className: 'code', data: 'KDTreeSearchParamKNN(knn=n)' },
                    { type: 'Text', data: ", which limits the search to a fixed number of nearest neighbors " },
                    { type: 'span', className: 'code', data: 'n' },
                    { type: 'Text', data: " without a radius constraint." },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "I experimented with the number of points in each point cloud, the search radius, and the nearest neighbours used for estimating normals. Initially, I intended to use " },
                    { type: 'span', className: 'code', data: '100,000' },
                    { type: 'Text', data: " points so that the normals were oriented properly. However, this took about 27 seconds per file (roughly 7 days to convert all files), hence I had to set it to " },
                    { type: 'span', className: 'code', data: '10,000' },
                    { type: 'Text', data: " points. To overcome normals being oriented consistently/properly with less number of points, there is an option in " },
                    { type: 'span', className: 'code', data: 'open3d' },
                    { type: 'Text', data: " called " },
                    { type: 'span', className: 'code', data: 'pcd.orient_normals_consistent_tangent_plane(k=n)' },
                    { type: 'Text', data: ", where n is the number of k nearest neighbors used in constructing the Riemannian graph used to propagate normal orientation. Adding this step took 4.36 seconds per file, which is the best result I got without relying on large number of points and ensuring that the normals are oriented properly/consistently. A search radius of " },
                    { type: 'span', className: 'code', data: '1' },
                    { type: 'Text', data: ", and " },
                    { type: 'span', className: 'code', data: '30' },
                    { type: 'Text', data: " nearest neighbors were set for the search parameter " },
                    { type: 'span', className: 'code', data: 'KDTreeSearchParamHybrid(radius=1, max_nn=30)' },
                    { type: 'Text', data: ". All the converted data are saved in a " },
                    { type: 'span', className: 'code', data: '.txt' },
                    { type: 'Text', data: " files and stored in " },
                    { type: 'span', className: 'code', data: 'MFD_dataset/pcd' },
                    { type: 'Text', data: " folder. Each file contains the PCD, and if normals are included, the file will store both point coordinates " },
                    { type: 'span', className: 'code', data: '(x, y, z)' },
                    { type: 'Text', data: " and normal vectors " },
                    { type: 'span', className: 'code', data: '(nx, ny, nz)' },
                    { type: 'Text', data: " in a comma-separated format." },
                ]
            },
            { type: 'Collapsable', colType: 'information', title: 'Processing time for converting STL to PCD', extraText: "The conversion of all 24,000 STL files into unprocessed raw PCD files with consistently oriented normals required approximately 29 hours. To optimize the processing time, I recommend experimenting with the parameters if you want to further reduce the processing time." },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "To visualize the point cloud with normals, you can use the class in " },
                    { type: 'span', className: 'code', data: 'visualizers/PreprocessingPCD.py' },
                    { type: 'Text', data: " script called " },
                    { type: 'span', className: 'code', data: 'VisualizePCD_Preprocessing' },
                    { type: 'Text', data: ". Please refer to " },
                    { type: 'Link', to: "#sixth_first", className: 'blog_links', data: "this section" },
                    { type: 'Text', data: " for details." },
                ]
            },

            // Second-Second Section - Preprocessing PCD (Optional)
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "This is an optional step, which is to preprocess the unprocessed raw PCD by applying Farthest Point Sampling (FPS) algorithm and normalizing them to be zero mean and within a unit ball. The preprocessed data are stored in " },
                    { type: 'span', className: 'code', data: 'MFD_dataset/preprocessed_pcd_x' },
                    { type: 'Text', data: " folder where " },
                    { type: 'span', className: 'code', data: 'x' },
                    { type: 'Text', data: " is the user specified number of points used for training the model. The function " },
                    { type: 'span', className: 'code', data: 'preprocess_pcd' },
                    { type: 'Text', data: " can be found in " },
                    { type: 'span', className: 'code', data: 'utils/utils.py' },
                    { type: 'Text', data: " script. While preprocessing all 24,000 PCD files takes approximately one hour on my machine, I strongly advise keeping the " },
                    { type: 'span', className: 'code', data: '--preprocess_data' },
                    { type: 'Text', data: " argument set to " },
                    { type: 'span', className: 'code', data: 'True' },
                    { type: 'Text', data: " (which is the default). Doing so ensures you only run this once; otherwise, the system will be forced to re-process the raw data on the fly every single time you run a training session, test, or visualizer." },
                ]
            },
            {
                type: 'CodeBlock', title: 'Python', extraText: [
                    [{ type: 'e', code: '# Import global modules' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' numpy ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' np' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' pathlib ' }, { type: 'a', code: 'import' }, { type: 'b', code: ' Path' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' tqdm ' }, { type: 'a', code: 'import' }, { type: 'b', code: ' tqdm' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' fpsample ' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' pickle' }],
                    [{ type: '', code: '' }],

                    // Function Definition
                    [{ type: 'a', code: 'def' }, { type: 'g', code: ' preprocess_pcd' }, { type: 'b', code: '(root: ' }, { type: 'b', code: 'Path, num_points: ' }, { type: 'l', code: 'int' }, { type: 'b', code: ' = ' }, { type: 'h', code: '1024' }, { type: 'b', code: ') -> ' }, { type: 'a', code: 'None' }, { type: 'b', code: ':' }],

                    // Docstring
                    [{ type: 'd', code: '    """' }],
                    [{ type: 'd', code: '    Processes point cloud data by optionally applying farthest point sampling or selecting a specified number of points' }],
                    [{ type: 'd', code: '    from the point cloud, followed by normalizing the points to fit within a unit sphere centered at the origin.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :param root: Path of the PointNet2_MFD folder' }],
                    [{ type: 'd', code: '    :param num_points: Number of training/testing points' }],
                    [{ type: 'd', code: '    :return: None' }],
                    [{ type: 'd', code: '    """' }],

                    // pcd_folder
                    [{ type: 'b', code: '    pcd_folder = root / ' }, { type: 'c', code: "'MFD_dataset/pcd'" }],
                    [{ type: '', code: '' }],

                    // preprocessed_data_folder setup
                    [{ type: 'b', code: '    preprocessed_data_folder = root / ' }, { type: 'c', code: "f'MFD_dataset/preprocessed_pcd_{" }, { type: 'b', code: 'num_points' }, { type: 'c', code: "}'" }],
                    [{ type: 'b', code: '    preprocessed_data_folder.mkdir(' }, { type: 'f', code: 'parents' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ', ' }, { type: 'f', code: 'exist_ok' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    // Outer Loop
                    [{ type: 'a', code: '    for' }, { type: 'b', code: ' folders ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' tqdm(pcd_folder.iterdir(), ' }, { type: 'f', code: 'desc' }, { type: 'b', code: '=' }, { type: 'c', code: "f'Applying FPS with " }, { type: 'a', code: "{"}, { type: 'b', code: 'num_points'}, { type: 'a', code: "}"}, { type: 'c', code: " points'"}, { type: 'b', code: ', ' }, { type: 'f', code: 'total' }, { type: 'b', code: '=' }, { type: 'l', code: 'len'}, { type: 'b', code: '(os.listdir(pcd_folder))):' }],
                    [{ type: 'b', code: '        subfolder_path = preprocessed_data_folder / folders.stem' }, { type: 'e', code: '  # Create subdirectory for the folder in pcd' }],
                    [{ type: 'b', code: '        subfolder_path.mkdir(' }, { type: 'f', code: 'parents' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ', ' }, { type: 'f', code: 'exist_ok' }, { type: 'b', code: '=' }, { type: 'a', code: 'True' }, { type: 'b', code: ')' }, { type: 'e', code: '  # Create the subfolder if it doesn\'t exist' }],
                    [{ type: '', code: '' }],

                    // Inner Loop
                    [{ type: 'a', code: '        for' }, { type: 'b', code: ' files ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' folders.iterdir():' }],
                    [{ type: 'b', code: '            save_path = subfolder_path / ' }, { type: 'c', code: 'f"{' }, { type: 'b', code: 'files.stem' }, { type: 'c', code: '}.dat"' }],
                    [{ type: '', code: '' }],

                    // Check if exists
                    [{ type: 'e', code: '            # Skip processing if the output file already exists' }],
                    [{ type: 'a', code: '            if' }, { type: 'b', code: ' save_path.exists():' }],
                    [{ type: 'a', code: '                continue' }],
                    [{ type: '', code: '' }],

                    // Load Data
                    [{ type: 'e', code: '            # Reads the PCD text file and extracts the points' }],
                    [{ type: 'b', code: '            pcd = np.loadtxt(files, ' }, { type: 'f', code: 'delimiter' }, { type: 'b', code: '=' }, { type: 'c', code: "','" }, { type: 'b', code: ').astype(np.float32)' }],
                    [{ type: '', code: '' }],

                    // FPS Sampling
                    [{ type: 'e', code: '            # Apply farthest point sampling for uniform sampling of PCD' }],
                    [{ type: 'b', code: '            sampled_pcd = pcd[fpsample.fps_sampling(' }, { type: 'f', code: 'pc' }, { type: 'b', code: '=pcd[:, :' }, { type: 'h', code: '3' }, { type: 'b', code: '], ' }, { type: 'f', code: 'n_samples' }, { type: 'b', code: '=num_points)]' }],
                    [{ type: '', code: '' }],

                    // Normalization
                    [{ type: 'e', code: '            # Normalize the point sets of \'xyz\' so they are between -1 and 1' }],
                    [{ type: 'b', code: '            sampled_pcd[:, ' }, { type: 'h', code: '0' }, { type: 'b', code: ':' }, { type: 'h', code: '3' }, { type: 'b', code: '] = normalize_pcd(' }, { type: 'f', code: 'point_cloud' }, { type: 'b', code: '=sampled_pcd[:, :' }, { type: 'h', code: '3' }, { type: 'b', code: '])' }],
                    [{ type: '', code: '' }],

                    // Save Pickle
                    [{ type: 'a', code: '            with' }, { type: 'l', code: ' open' }, { type: 'b', code: '(save_path, ' }, { type: 'c', code: "'wb'" }, { type: 'b', code: ') ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' f:' }],
                    [{ type: 'b', code: '                pickle.dump(sampled_pcd, f)' }],
                    [{ type: '', code: '' }],

                    // Print Statement
                    [{ type: 'l', code: '    print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Preprocessed point clouds by sampling to " }, {type: 'a', code: '{'}, { type: 'b', code: 'num_points' }, {type: 'a', code: '}'}, { type: 'c', code: " and normalizing'" }, { type: 'b', code: ')' }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "The function first loads each of the " },
                    { type: 'span', className: 'code', data: '.txt' },
                    { type: 'Text', data: " files from the " },
                    { type: 'span', className: 'code', data: 'MFD_dataset/pcd' },
                    { type: 'Text', data: " folder and applies FPS algorithm using a function from library called " },
                    { type: 'span', className: 'code', data: 'fpsample' },
                    { type: 'Text', data: ". Only the point coordinates " },
                    { type: 'span', className: 'code', data: '(x, y, z)' },
                    { type: 'Text', data: " were passed into FPS algorithm, and the number of points was set to " },
                    { type: 'span', className: 'code', data: '1024' },
                    { type: 'Text', data: ", as outlined in the original PointNet++ paper. However, the user has the flexibility to specify a different number of points if needed. Keep in mind that the number of points selected for FPS should be lower than the total number of points in the PCD or else it will throw you an error. The down-sampled data is then normalized to be zero mean and within a unit ball. This normalized point coordinates with their respective normals " },
                    { type: 'span', className: 'code', data: '(nx, ny, nz)' },
                    { type: 'Text', data: " are then saved to file using " },
                    { type: 'span', className: 'code', data: 'pickle' },
                    { type: 'Text', data: " module." },
                    { type: 'Text', data: "You can also visualize PCD with normals preprocessing using the class " },
                    { type: 'span', className: 'code', data: 'VisualizePCD_Preprocessing' },
                    { type: 'Text', data: " in the " },
                    { type: 'span', className: 'code', data: 'visualizers/PreprocessingPCD.py' },
                    { type: 'Text', data: " script. Please refer to " },
                    { type: 'Link', to: "#sixth_first", className: 'blog_links', data: "this section" },
                    { type: 'Text', data: " for details." },
                ]
            },

            // Third Section - Data Loading and Preparation
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "The " },
                    { type: 'span', className: 'code', data: 'PointNet2DataLoader' },
                    { type: 'Text', data: " class in " },
                    { type: 'span', className: 'code', data: 'utils/PointNet2DataLoader.py' },
                    { type: 'Text', data: " script is responsible for loading and preparing the dataset for training and testing. It handles loading preprocessed PCD (if it's preprocessed), splitting dataset, and shuffling." },
                ]
            },
            {
                type: 'CodeBlock', title: "Python", extraText: [
                    [{ type: 'e', code: '# Import global modules' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' argparse' }, { type: 'a', code: ' import' }, { type: 'b', code: ' Namespace' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' numpy' }, { type: 'a', code: ' as' }, { type: 'b', code: ' np' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' pickle' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' tqdm' }, { type: 'a', code: ' import' }, { type: 'b', code: ' tqdm' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' torch.utils.data' }, { type: 'a', code: ' import' }, { type: 'b', code: ' Dataset' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' fpsample' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' sklearn.model_selection' }, { type: 'a', code: ' import' }, { type: 'b', code: ' train_test_split' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' pathlib' }, { type: 'a', code: ' import' }, { type: 'b', code: ' Path' }],
                    [{ type: 'a', code: 'import' }, { type: 'b', code: ' os' }],
                    [{ type: '', code: '' }],
                    [{ type: 'e', code: '# Import local modules' }],
                    [{ type: 'a', code: 'from' }, { type: 'b', code: ' utils.utils' }, { type: 'a', code: ' import' }, { type: 'b', code: ' normalize_pcd' }],
                    [{ type: '', code: '' }],

                    // Class Definition
                    [{ type: 'a', code: 'class' }, { type: 'b', code: ' PointNet2DataLoader' }, { type: 'b', code: '(Dataset):' }],

                    // Class Docstring
                    [{ type: 'd', code: '    """' }],
                    [{ type: 'd', code: '    A PyTorch Dataset class for loading and preprocessing point cloud data for PointNet++.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    Attributes:' }],
                    [{ type: 'd', code: '        data_folder: Path to the folder containing the point cloud data.' }],
                    [{ type: 'd', code: '        preprocess_data: Flag indicating whether the data is preprocessed.' }],
                    [{ type: 'd', code: '        num_points: Number of points to sample from each point cloud.' }],
                    [{ type: 'd', code: '        use_normals: Flag indicating whether to use normals in the point cloud data.' }],
                    [{ type: 'd', code: '        split: Dataset split, either \'train\' or \'test\'.' }],
                    [{ type: 'd', code: '        data: List of file paths for the dataset split.' }],
                    [{ type: 'd', code: '        list_of_points: List to store point cloud data.' }],
                    [{ type: 'd', code: '        list_of_labels: List to store corresponding labels.' }],
                    [{ type: 'd', code: '    """' }],

                    // __init__
                    [{ type: 'a', code: '    def' }, { type: 'k', code: ' __init__' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: ', data_folder: Path, args: Namespace, split: ' }, { type: 'l', code: 'str' }, { type: 'b', code: ' = ' }, { type: 'c', code: "'train'" }, { type: 'b', code: '):' }],
                    
                    // __init__ Docstring
                    [{ type: 'd', code: '        """' }],
                    [{ type: 'd', code: '        Initializes the PointNet2DataLoader.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '        :param data_folder: Path to the folder containing the point cloud data.' }],
                    [{ type: 'd', code: '        :param args: Arguments containing configuration parameters.' }],
                    [{ type: 'd', code: '        :param split: Dataset split, either \'train\' or \'test\'. Defaults to \'train\'.' }],
                    [{ type: 'd', code: '        """' }],

                    // Attributes assignment
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.data_folder = data_folder' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.preprocess_data = args.preprocess_data' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.num_points = args.num_points' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.use_normals = args.use_normals' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.split = split' }],
                    [{ type: '', code: '' }],

                    // Comment: Gather files
                    [{ type: 'e', code: '        # Gather all files grouped by class' }],
                    [{ type: 'b', code: '        folders = ' }, { type: 'l', code: 'sorted' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data_folder.iterdir(), ' }, { type: 'f', code: 'key'}, {type: 'b', code: '='}, { type: 'a', code: 'lambda' }, { type: 'b', code: ' x: ' }, { type: 'l', code: 'int' }, { type: 'b', code: '(x.stem.split(' }, { type: 'c', code: "'_'" }, { type: 'b', code: ')[' }, { type: 'h', code: '0' }, { type: 'b', code: ']))' }],
                    
                    // Files array list comprehension
                    [{ type: 'b', code: '        files = np.array([' }, { type: 'l', code: 'sorted' }, { type: 'b', code: '(folder.rglob(' }, { type: 'c', code: '"*"' }, { type: 'b', code: '), ' }, { type: 'f', code: 'key'}, {type: 'b', code: '='}, { type: 'a', code: 'lambda' }, { type: 'b', code: ' x: ' }, { type: 'l', code: 'int' }, { type: 'b', code: '(x.stem)) ' }, { type: 'a', code: 'for' }, { type: 'b', code: ' folder ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' folders])' }],
                    [{ type: '', code: '' }],

                    // Comment: Init lists
                    [{ type: 'e', code: '        # Initialize lists to hold training and testing sets' }],
                    [{ type: 'b', code: '        train_files, test_files = [], []' }],
                    [{ type: '', code: '' }],

                    // Comment: Split loop
                    [{ type: 'e', code: '        # Split files for each class with a fixed random seed...' }],
                    [{ type: 'e', code: '        # folder is not arbitrary (for example, if the file numbers correlate with some underlying trend)' }],
                    [{ type: 'a', code: '        for' }, { type: 'b', code: ' i, class_files ' }, { type: 'a', code: 'in' }, { type: 'l', code: ' enumerate' }, { type: 'b', code: '(files):' }],
                    [{ type: 'b', code: '            train, test = train_test_split(class_files, ' }, { type: 'f', code: 'test_size'}, {type: 'b', code: '='}, { type: 'h', code: '0.2' }, { type: 'b', code: ', ' }, { type: 'f', code: 'shuffle'}, {type: 'b', code: '='}, { type: 'a', code: 'True' }, { type: 'b', code: ', ' }, { type: 'f', code: 'random_state'}, {type: 'b', code: '='}, { type: 'h', code: '42' }, { type: 'b', code: '+i)' }],
                    [{ type: 'b', code: '            train_files.extend(train)' }],
                    [{ type: 'b', code: '            test_files.extend(test)' }],
                    [{ type: '', code: '' }],

                    // Comment: Save paths
                    [{ type: 'e', code: '        # Save the train and test file paths to txt files...' }],
                    [{ type: 'b', code: '        log_path = Path.cwd() / ' }, { type: 'c', code: "f'Logs/{" }, { type: 'b', code: 'args.save_path' }, { type: 'c', code: "}'" }, { type: 'a', code: ' if' }, { type: 'b', code: ' args.save_path ' }, { type: 'a', code: 'else' }, { type: 'b', code: ' Path.cwd() / ' }, { type: 'c', code: "f'Logs/{" }, { type: 'b', code: 'args.model' }, { type: 'c', code: "}'" }],
                    [{ type: '', code: '' }],

                    // Write Train Files
                    [{ type: 'b', code: '        train_txt = log_path / ' }, { type: 'c', code: "'train_files.txt'" }],
                    [{ type: 'a', code: '        with' }, { type: 'l', code: ' open' }, { type: 'b', code: '(train_txt, ' }, { type: 'c', code: "'w'" }, { type: 'b', code: ') ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' f:' }],
                    [{ type: 'a', code: '            for' }, { type: 'b', code: ' file ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' train_files:' }],
                    [{ type: 'b', code: '                f.write(' }, { type: 'l', code: 'str' }, { type: 'b', code: '(os.path.join(*file.parts[' }, { type: 'h', code: '-2' }, { type: 'b', code: ':])) + ' }, { type: 'c', code: "'\\n'" }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    // Write Test Files
                    [{ type: 'b', code: '        test_txt = log_path / ' }, { type: 'c', code: "'test_files.txt'" }],
                    [{ type: 'a', code: '        with' }, { type: 'l', code: ' open' }, { type: 'b', code: '(test_txt, ' }, { type: 'c', code: "'w'" }, { type: 'b', code: ') ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' f:' }],
                    [{ type: 'a', code: '            for' }, { type: 'b', code: ' file ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' test_files:' }],
                    [{ type: 'b', code: '                f.write(' }, { type: 'l', code: 'str' }, { type: 'b', code: '(os.path.join(*file.parts[' }, { type: 'h', code: '-2' }, { type: 'b', code: ':])) + ' }, { type: 'c', code: "'\\n'" }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    // Prints
                    [{ type: 'l', code: '        print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Train files information saved in: {" }, { type: 'b', code: 'train_txt' }, { type: 'c', code: "}'" }, { type: 'b', code: ')' }],
                    [{ type: 'l', code: '        print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Test files information saved in: {" }, { type: 'b', code: 'test_txt' }, { type: 'c', code: "}'" }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    // Assert and Assign Split
                    [{ type: 'e', code: '        # Ensure the split is either \'train\' or \'test\', and selects the files for further processing' }],
                    [{ type: 'a', code: '        assert' }, { type: 'b', code: ' (' }, { type: 'i', code: 'self' }, { type: 'b', code: '.split == ' }, { type: 'c', code: "'train'" }, { type: 'a', code: ' or' }, { type: 'i', code: ' self' }, { type: 'b', code: '.split == ' }, { type: 'c', code: "'test'" }, { type: 'b', code: ')' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.data = train_files ' }, { type: 'a', code: 'if' }, { type: 'i', code: ' self' }, { type: 'b', code: '.split == ' }, { type: 'c', code: "'train'" }, { type: 'a', code: ' else' }, { type: 'b', code: ' test_files' }],
                    [{ type: 'l', code: '        print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'The size of {" }, { type: 'b', code: 'split' }, { type: 'c', code: "} data is {" }, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)}' }, { type: 'c', code: "'" }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    // Init data holders
                    [{ type: 'e', code: '        # Initialize lists to store point clouds and labels' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.list_of_points = [' }, { type: 'a', code: 'None' }, { type: 'b', code: '] * ' }, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)' }],
                    [{ type: 'i', code: '        self' }, { type: 'b', code: '.list_of_labels = [' }, { type: 'a', code: 'None' }, { type: 'b', code: '] * ' }, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)' }],
                    [{ type: '', code: '' }],

                    // If/Else Processing Block
                    [{ type: 'e', code: '        # If data is preprocessed, load it. Else, process it before training' }],
                    [{ type: 'a', code: '        if' }, { type: 'i', code: ' self' }, { type: 'b', code: '.preprocess_data:' }],
                    [{ type: 'l', code: '            print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Loading processed data from {" }, { type: 'i', code: 'self' }, { type: 'b', code: '.data_folder' }, { type: 'c', code: "}'" }, { type: 'b', code: ')' }],
                    [{ type: 'a', code: '            for' }, { type: 'b', code: ' index ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' tqdm(' }, { type: 'l', code: 'range' }, { type: 'b', code: '(' }, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)), ' }, { type: 'f', code: 'total'}, {type: 'b', code: '='}, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)):' }],
                    [{ type: 'a', code: '                with' }, { type: 'l', code: ' open' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data[index], ' }, { type: 'c', code: "'rb'" }, { type: 'b', code: ') ' }, { type: 'a', code: 'as' }, { type: 'b', code: ' f:' }],
                    [{ type: 'i', code: '                    self' }, { type: 'b', code: '.list_of_points[index] = pickle.load(f)' }],
                    [{ type: 'i', code: '                    self' }, { type: 'b', code: '.list_of_labels[index] = ' }, { type: 'l', code: 'int' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data[index].stem.split(' }, { type: 'c', code: "'_'" }, { type: 'b', code: ')[' }, { type: 'h', code: '0' }, { type: 'b', code: '])' }],
                    
                    // Else Block
                    [{ type: 'a', code: '        else' }, { type: 'b', code: ':' }],
                    [{ type: 'l', code: '            print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Preprocessing data from {" }, { type: 'i', code: 'self' }, { type: 'b', code: '.data_folder' }, { type: 'c', code: "}'" }, { type: 'b', code: ')' }],
                    [{ type: 'a', code: '            for' }, { type: 'b', code: ' index ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' tqdm(' }, { type: 'l', code: 'range' }, { type: 'b', code: '(' }, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)), ' }, { type: 'f', code: 'total'}, {type: 'b', code: '='}, { type: 'l', code: 'len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)):' }],
                    [{ type: 'e', code: '                # reads the text file and extracts the points' }],
                    [{ type: 'b', code: '                pcd = np.loadtxt(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data[index], delimiter=' }, { type: 'c', code: "','" }, { type: 'b', code: ').astype(np.float32)' }],
                    [{ type: '', code: '' }],
                    [{ type: 'e', code: '                # Apply farthest point sampling for uniform sampling of PCD' }],
                    [{ type: 'b', code: '                sampled_pcd = pcd[fpsample.fps_sampling('}, {type: 'f', code: 'pc'}, {type: 'b', code: '='}, {type: 'b', code: 'pcd[:, :'}, { type: 'h', code: '3' }, { type: 'b', code: '], ' }, {type: 'f', code: 'nsamples'}, {type: 'b', code: '='}, { type: 'i', code: 'self' }, { type: 'b', code: '.num_points)]' }],
                    [{ type: '', code: '' }],
                    [{ type: 'e', code: '                # Normalize the point sets of \'xyz\' so they are between -1 and 1' }],
                    [{ type: 'b', code: '                sampled_pcd[:, ' }, { type: 'h', code: '0' }, { type: 'b', code: ':' }, { type: 'h', code: '3' }, { type: 'b', code: '] = normalize_pcd('}, {type: 'f', code: 'point_cloud'}, {type: 'b', code: '=sampled_pcd[:, :' }, { type: 'h', code: '3' }, { type: 'b', code: '])' }],
                    [{ type: '', code: '' }],
                    [{ type: 'i', code: '                self' }, { type: 'b', code: '.list_of_points[index] = sampled_pcd' }],
                    [{ type: 'i', code: '                self' }, { type: 'b', code: '.list_of_labels[index] = ' }, { type: 'l', code: 'int' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data[index].parent.stem.split(' }, { type: 'c', code: "'_'" }, { type: 'b', code: ')[' }, { type: 'h', code: '0' }, { type: 'b', code: '])' }],
                    [{ type: '', code: '' }],

                    // __len__
                    [{ type: 'a', code: '    def' }, { type: 'k', code: ' __len__' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: ') -> ' }, { type: 'l', code: 'int' }, { type: 'b', code: ':' }],
                    [{ type: 'd', code: '        """Returns the number of samples in the dataset"""' }],
                    [{ type: 'a', code: '        return' }, { type: 'l', code: ' len' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: '.data)' }],
                    [{ type: '', code: '' }],

                    // __getitem__
                    [{ type: 'a', code: '    def' }, { type: 'k', code: ' __getitem__' }, { type: 'b', code: '(' }, { type: 'i', code: 'self' }, { type: 'b', code: ', index: ' }, { type: 'l', code: 'int' }, { type: 'b', code: ') -> ' }, { type: 'l', code: 'tuple' }, { type: 'b', code: '[np.ndarray, ' }, { type: 'l', code: 'int' }, { type: 'b', code: ']:' }],
                    [{ type: 'd', code: '        """Loads and returns a sample from the dataset at the given index idx"""' }],
                    [{ type: 'b', code: '        point_set, label = ' }, { type: 'i', code: 'self' }, { type: 'b', code: '.list_of_points[index], ' }, { type: 'i', code: 'self' }, { type: 'b', code: '.list_of_labels[index]' }],
                    [{ type: '', code: '' }],
                    [{ type: 'e', code: '        # If normals are not used, only return the xyz coordinates' }],
                    [{ type: 'b', code: '        point_set = point_set[:, :' }, { type: 'h', code: '3' }, { type: 'b', code: '] ' }, { type: 'a', code: 'if' }, { type: 'a', code: ' not' }, { type: 'i', code: ' self' }, { type: 'b', code: '.use_normals ' }, { type: 'a', code: 'else' }, { type: 'b', code: ' point_set' }],
                    [{ type: '', code: '' }],
                    [{ type: 'a', code: '        return' }, { type: 'b', code: ' point_set, label' }],
                    
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: "According to PointNet++ paper " },
                    { type: 'Link', to: '#ref_1', className: 'blog_links', data: [{ type: 'Text', data: '[1]' }] },
                    { type: 'Text', data: ", the dataset is split so that " },
                    { type: 'span', className: 'code', data: '80%' },
                    { type: 'Text', data: " of PCD is used for training and " },
                    { type: 'span', className: 'code', data: '20%' },
                    { type: 'Text', data: " for testing. If PCD is not preprocessed, using the same principle as " },
                    { type: 'span', className: 'code', data: 'preprocess_pcd' },
                    { type: 'Text', data: " function, the data for training/testing will be preprocessed in the " },
                    { type: 'span', className: 'code', data: '__init__' },
                    { type: 'Text', data: " so that its not repeating each time a batch is fetched. This class returns a point set consisting of point coordinates " },
                    { type: 'span', className: 'code', data: '(x, y, z)' },
                    { type: 'Text', data: " and, if available, normal vectors " },
                    { type: 'span', className: 'code', data: '(nx, ny, nz)' },
                    { type: 'Text', data: ", along with a label representing the type of machining feature." },
                ]
            },
            { type: 'Collapsable', colType: 'information', title: 'Dataset split', extraText: "Each machining feature dataset folder contains 1000 files following a specific naming convention. Since we are unsure whether the file order in each folder is entirely arbitrary or if the file numbers correlate with an underlying trend, we use a careful splitting strategy. For each class, the files are shuffled with a fixed random seed to ensure reproducibility while allowing the random shuffling to differ across classes. This approach helps prevent any potential bias that might come from a systematic file order, which will ensure a robust and representative split for training and testing." },

            // Fourth Section - Training and Evaluating the Model
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The next step is to train the PointNet++ model. There are three model variants: SSG (' },
                    { type: 'span', className: 'code', data: 'PointNet2_CLS_SSG' },
                    { type: 'Text', data: '), MSG (' },
                    { type: 'span', className: 'code', data: 'PointNet2_CLS_MSG' },
                    { type: 'Text', data: '), and MRG (' },
                    { type: 'span', className: 'code', data: 'PointNet2_CLS_MRG' },
                    { type: 'Text', data: '). For each variant, you can train with or without random input dropout. To minimally train the model without changing the parameters, you can run the following shell command below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python train_classification.py --save_path your_folder_name --random_dropout --model PointNet2_CLS_SSG" }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The ' },
                    { type: 'span', className: 'code', data: 'train_classification' },
                    { type: 'Text', data: ' function in ' },
                    { type: 'span', className: 'code', data: 'train_classification.py' },
                    { type: 'Text', data: ' starts off by selecting a computation device, CPU or GPU, to be used when training the model. Next, logger files are then setup: one for temporary use and one for the final version. The temporary logger file will later be deleted at the end of the training session and is just used to store training data per epoch and other relevant information. The final logger file, on the other hand, will neatly organize and tabulate all the data at the end of the training session. I used a library called ' },
                    { type: 'span', className: 'code', data: 'PrettyTable' },
                    { type: 'Text', data: ' for this, and its rather pretty neat. The next two steps are ' },
                    { type: 'Link', to: "#second_first", className: 'blog_links', data: "converting STL to PCD" },
                    { type: 'Text', data: ' and ' },
                    { type: 'Link', to: "#second_second", className: 'blog_links', data: "preprocessing PCD" },
                    { type: 'Text', data: '. Then the ' },
                    { type: 'Link', to: "#third", className: 'blog_links', data: "data loading and preparation step" },
                    { type: 'Text', data: ' is initialized.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'There are three models that users can choose for PointNet++: SSG, MSG, and MRG. Based on the users choice, the chosen model, the loss criterion (negative log-likelihood loss function), and the optimizer is initialized. Though the paper did not specify what optimizer is used for training the model, in their supplementary materials they mentioned the use of Adam optimizer. The Learning Rate (LR) is set to ' },
                    { type: 'span', className: 'code', data: '0.001' },
                    { type: 'Text', data: '. For the optimizer, based on the ' },
                    { type: 'Link', to: "https://github.com/charlesq34/pointnet2", className: 'blog_links', data: "original PointNet++ code" },
                    { type: 'Text', data: ', the authors used exponential decay for learning rate. To mimic the TensorFlow version, a small lambda function for LR scheduling is defined. The exponential decay for the learning rate is as follows:' },
                ]
            },
            { type: 'img', id: 'fig_2', className: 'Blog_Image', data: [Figure2, 'Figure 2: Learning Rate Decay Over Training', ''] },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'I added a step to check if there are any previous existing trained model, and if so, it loads the best model checkpoint and uses it to test the model. If the best model doesnt exist, then it checks for the last saved checkpoint and if the training is complete (i.e., current epoch is equal to number of epochs used to train), then it uses this checkpoint to test the model. However, if no checkpoints exist, then it starts training from scratch.' },
                ]
            },

            // Fourth-First Section - Training Loop
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
{
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The script then runs the ' },
                    { type: 'span', className: 'code', data: 'train_one_epoch' },
                    { type: 'Text', data: ' function for each epoch:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Python', extraText: [
                    // Function Definition
                    [{ type: 'a', code: 'def' }, { type: 'g', code: ' train_one_epoch' }, { type: 'b', code: '(' }],
                    [{ type: 'b', code: '        epoch: ' }, { type: 'l', code: 'int' }, { type: 'b', code: ', model: torch.nn.Module, criterion: torch.nn.Module, optimizer: torch.optim.Optimizer,' }],
                    [{ type: 'b', code: '        scheduler: torch.optim.lr_scheduler.LRScheduler, data_loader: torch.utils.data.DataLoader,' }],
                    [{ type: 'b', code: '        device: torch.device, args: argparse.Namespace' }],
                    [{ type: 'b', code: ') -> ' }, { type: 'l', code: 'float' }, { type: 'b', code: ':' }],

                    // Docstring
                    [{ type: 'd', code: '    """' }],
                    [{ type: 'd', code: '    Train the model for one epoch and return the average training accuracy across all batches.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :param epoch: Current epoch number.' }],
                    [{ type: 'd', code: '    :param model: The PointNet++ model to be trained.' }],
                    [{ type: 'd', code: '    :param criterion: The loss function used to compute model loss.' }],
                    [{ type: 'd', code: '    :param optimizer: Optimizer for updating model weights.' }],
                    [{ type: 'd', code: '    :param scheduler: Learning rate scheduler for dynamic adjustment.' }],
                    [{ type: 'd', code: '    :param data_loader: DataLoader for the training dataset.' }],
                    [{ type: 'd', code: '    :param device: The device (CPU/GPU) where training is executed.' }],
                    [{ type: 'd', code: '    :param args: Parsed command-line arguments containing training parameters.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :return: The average training accuracy for the epoch.' }],
                    [{ type: 'd', code: '    """' }],
                    [{ type: '', code: '' }],

                    // Body
                    [{ type: 'b', code: '    mean_correct = []' }, { type: 'e', code: '  # List to store accuracies of each batch in the epoch' }],
                    [{ type: 'b', code: '    classifier = model.train()' }, { type: 'e', code: '  # Set model to train mode' }],
                    [{ type: '', code: '' }],

                    // Loop (Updated tqdm to 'b' and {} to 'a')
                    [{ type: 'a', code: '    for' }, { type: 'b', code: ' batch_idx, (points, target) ' }, { type: 'a', code: 'in' }, { type: 'l', code: ' enumerate' }, { type: 'b', code: '(' }, { type: 'b', code: 'tqdm' }, { type: 'b', code: '(data_loader, ' }, { type: 'f', code: 'desc' }, { type: 'b', code: '=' }, { type: 'c', code: 'f"Epoch ' }, { type: 'a', code: '{' }, { type: 'b', code: 'epoch' }, { type: 'h', code: '+1' }, { type: 'a', code: '}' }, { type: 'c', code: '/' }, { type: 'a', code: '{' }, { type: 'b', code: 'args.max_epoch' }, { type: 'a', code: '}' }, { type: 'c', code: '"' }, { type: 'b', code: ')):' }],
                    [{ type: 'b', code: '        optimizer.zero_grad()' }],
                    [{ type: '', code: '' }],

                    // Augmentation Block
                    [{ type: 'a', code: '        if' }, { type: 'b', code: ' args.augment_data:' }],
                    [{ type: 'b', code: '            points = AugmentData(' }, { type: 'f', code: 'batch_data' }, { type: 'b', code: '=points, ' }, { type: 'f', code: 'use_normals' }, { type: 'b', code: '=args.use_normals).augment_batch_data(' }, { type: 'f', code: 'dropout_points' }, { type: 'b', code: '=args.random_dropout)' }],
                    [{ type: '', code: '' }],

                    // Transpose & Device
                    [{ type: 'e', code: '        # Original shape: (batch_size, num_features, num_points), New shape: (batch_size, num_points, num_features)' }],
                    [{ type: 'b', code: '        points = points.transpose(' }, { type: 'h', code: '2' }, { type: 'b', code: ', ' }, { type: 'h', code: '1' }, { type: 'b', code: ')' }],
                    [{ type: 'b', code: '        points, target = points.to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device), target.to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device)' }],
                    [{ type: '', code: '' }],

                    // Batch Norm Momentum
                    [{ type: 'e', code: '        # Adjust batch norm momentum' }],
                    [{ type: 'b', code: '        global_batch = epoch * ' }, { type: 'l', code: 'len' }, { type: 'b', code: '(data_loader) + batch_idx' }],
                    [{ type: 'b', code: '        bn_momentum = get_bn_decay(' }],
                    [{ type: 'b', code: '            ' }, { type: 'f', code: 'global_batch' }, { type: 'b', code: '=global_batch, ' }, { type: 'f', code: 'batch_size' }, { type: 'b', code: '=args.batch_size, ' }, { type: 'f', code: 'decay_step' }, { type: 'b', code: '=args.decay_step,' }],
                    [{ type: 'b', code: '            ' }, { type: 'f', code: 'bn_init_decay' }, { type: 'b', code: '=args.bn_init_decay, ' }, { type: 'f', code: 'bn_decay_decay_rate' }, { type: 'b', code: '=args.bn_decay_decay_rate,' }],
                    [{ type: 'b', code: '            ' }, { type: 'f', code: 'bn_decay_clip' }, { type: 'b', code: '=args.bn_decay_clip' }],
                    [{ type: 'b', code: '        )' }],
                    [{ type: 'b', code: '        assign_bn_momentum(model, bn_momentum)' }],
                    [{ type: '', code: '' }],

                    // Forward & Loss
                    [{ type: 'b', code: '        pred, trans_feat = classifier(points)' }, { type: 'e', code: '  # Forward pass through the model' }],
                    [{ type: 'b', code: '        loss = criterion(pred, target.long())' }, { type: 'e', code: '  # Compute loss' }],
                    [{ type: '', code: '' }],

                    // Backward
                    [{ type: 'e', code: '        # Backpropagate loss' }],
                    [{ type: 'b', code: '        loss.backward()' }],
                    [{ type: 'b', code: '        optimizer.step()' }],
                    [{ type: 'b', code: '        scheduler.step()' }, { type: 'e', code: '  # Update LR based on LambdaLR' }],
                    [{ type: '', code: '' }],

                    // Metrics
                    [{ type: 'b', code: '        pred_choice = pred.data.max(' }, { type: 'h', code: '1' }, { type: 'b', code: ')[' }, { type: 'h', code: '1' }, { type: 'b', code: ']' }, { type: 'e', code: '  # Get predicted class with highest score' }],
                    [{ type: 'b', code: '        correct = pred_choice.eq(target.long().data).cpu().sum()' }, { type: 'e', code: '  # Count correct predictions' }],
                    [{ type: 'b', code: '        mean_correct.append(correct.item() / ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(points.size()[' }, { type: 'h', code: '0' }, { type: 'b', code: ']))' }, { type: 'e', code: '  # Calculate batch accuracy' }],
                    [{ type: '', code: '' }],

                    // Return (Updated {} to 'a')
                    [{ type: 'e', code: '    # Return mean accuracy over all batches in this epoch' }],
                    [{ type: 'b', code: '    train_instance_acc = ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(np.mean(mean_correct))' }],
                    [{ type: 'l', code: '    print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Train Instance Accuracy: " }, { type: 'a', code: '{' }, { type: 'b', code: 'train_instance_acc' }, { type: 'a', code: '}' }, { type: 'c', code: "'" }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],
                    [{ type: 'a', code: '    return' }, { type: 'b', code: ' train_instance_acc' }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'For each batch of data points fetched from the ' },
                    { type: 'span', className: 'code', data: 'train_data_loader' },
                    { type: 'Text', data: ', PCD data augmentation is applied to it. Based on the ' },
                    { type: 'Link', to: "https://github.com/charlesq34/pointnet2", className: 'blog_links', data: "original PointNet++ code" },
                    { type: 'Text', data: ', the data augmentation applied to each PCD in the batch are in the following order:' },
                ]
            },
            {
                type: 'ol', className: 'body', data: [
                    { type: 'li', data: [{ type: 'Text', data: 'Randomly rotate the PCD around ' }, { type: 'span', className: 'code', data: 'Y' }, { type: 'Text', data: ' including the random rotation of normals between ' }, { type: 'span', className: 'code', data: '0' }, { type: 'Text', data: ' and ' }, { type: 'span', className: 'code', data: '2' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Randomly perturb the PCD by applying small rotations around the ' }, { type: 'span', className: 'code', data: 'X' }, { type: 'Text', data: ', ' }, { type: 'span', className: 'code', data: 'Y' }, { type: 'Text', data: ', and ' }, { type: 'span', className: 'code', data: 'Z' }, { type: 'Text', data: ' axes including the random perturbation of normals. A random rotation angle for each of the axes is generated by multiplying a normally distributed random value by user specified ' }, { type: 'span', className: 'code', data: 'sigma' }, { type: 'Text', data: ' which is the standard deviation of the rotation angles in radians and then clipping it to lie between user specified angle clip.' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Randomly scale the PCD in the ' }, { type: 'span', className: 'code', data: 'X' }, { type: 'Text', data: ', ' }, { type: 'span', className: 'code', data: 'Y' }, { type: 'Text', data: ', and ' }, { type: 'span', className: 'code', data: 'Z' }, { type: 'Text', data: ' axes' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Randomly shift the PCD in the ' }, { type: 'span', className: 'code', data: 'X' }, { type: 'Text', data: ', ' }, { type: 'span', className: 'code', data: 'Y' }, { type: 'Text', data: ', and ' }, { type: 'span', className: 'code', data: 'Z' }, { type: 'Text', data: ' axes' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Randomly add jitter (noise) to the PCD. The jitter is applied independently to each point. A random Gaussian noise is generated and added to the PCD' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Shuffle the order of points in PCD. The same shuffling index is applied across all PCD in the batch to ensure consistency. Shuffling the order of points can change the behaviour of FPS' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Apply random input dropout (refer to ' }, { type: 'Link', to: "https://blank-ed.github.io/ilyas_dawoodjee/#/blogpage/understanding_a_intuitive_visual#third", className: 'blog_links', data: "this section" }, { type: 'Text', data: ' for details) by randomly dropping points from the PCD. To simulate missing data, the dropped points are replaced with the first point in the batch so that they keep the same shape' }] },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'It is important to note that for steps 1 and 2, it is applied to both ' },
                    { type: 'span', className: 'code', data: 'XYZ' },
                    { type: 'Text', data: ' and normals. This is because if you only rotate the points, the normals remain in their original orientation. This misalignment means the normals no longer correctly represent the surface directions of the rotated geometry. However, steps 3 to 5 are only applied to ' },
                    { type: 'span', className: 'code', data: 'XYZ' },
                    { type: 'Text', data: ' because scaling, shifting, and jittering are geometric transformations that modify the positions of points in space. In contrast, surface normals, which indicate the orientation of a point cloud\'s surface, are directional vectors that must remain unit vectors (with a magnitude of ' },
                    { type: 'span', className: 'code', data: '1' },
                    { type: 'Text', data: ') and are therefore only rotated, not scaled or shifted, to preserve their properties.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'You can find the codes for data augmentation of PCD in the script ' },
                    { type: 'span', className: 'code', data: 'utils/PointNet2_Utils.py' },
                    { type: 'Text', data: ' with a class called ' },
                    { type: 'span', className: 'code', data: 'AugmentData' },
                    { type: 'Text', data: '. I have also created a visualization class ' },
                    { type: 'span', className: 'code', data: 'VisualizePCD_Augmentation' },
                    { type: 'Text', data: ' in ' },
                    { type: 'span', className: 'code', data: 'visualizers/Augmentation.py' },
                    { type: 'Text', data: ' script for you to play around with PCD augmentation. You can refer to ' },
                    { type: 'Link', to: "#sixth_second", className: 'blog_links', data: "this section" },
                    { type: 'Text', data: ' for more information.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'A dynamic Batch Normalization (BN) scheduling mechanism adjusts momentum throughout training, starting with higher momentum (' },
                    { type: 'span', className: 'code', data: '0.5' },
                    { type: 'Text', data: ') for stability during initial learning phases, then progressively reducing it to a minimum specified value to enhance final model refinement.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The forward pass through the network generates both classification predictions and transformed point features. The loss function combines cross-entropy classification error with any regularization terms from the network architecture. Backpropagation updates model parameters through the optimizer while the learning rate scheduler makes per-iteration adjustments following a predefined decay strategy. Instance-level accuracy is computed by comparing predicted class indices with ground truth labels, with batch accuracies aggregated and averaged across the epoch to produce the final training accuracy metric.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The learning rate scheduler update occurs after optimizer stepping to precisely replicate the original PointNet++ timing sequence. The accuracy metric follows the paper\'s instance-based calculation approach rather than class-balanced alternatives, which enables direct performance comparisons with published benchmarks. All augmentation operations are implemented as batched tensor manipulation rather than per-sample loops.' },
                ]
            },

            // Fourth-Second Section - Evaluation Loop
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The script then runs the ' },
                    { type: 'span', className: 'code', data: 'evaluate_one_epoch' },
                    { type: 'Text', data: ' function for each epoch:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Python', extraText: [
                    // Function Definition
                    [{ type: 'a', code: 'def' }, { type: 'g', code: ' evaluate_one_epoch' }, { type: 'b', code: '(' }],
                    [{ type: 'b', code: '        model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, device: torch.device, num_classes: ' }, { type: 'l', code: 'int' }],
                    [{ type: 'b', code: ') -> ' }, { type: 'l', code: 'tuple' }, { type: 'b', code: '[' }, { type: 'l', code: 'float' }, { type: 'b', code: ', ' }, { type: 'l', code: 'float' }, { type: 'b', code: ']:' }],
                    [{ type: '', code: '' }],

                    // Docstring
                    [{ type: 'd', code: '    """' }],
                    [{ type: 'd', code: '    Evaluate the model on a given data loader. Computes instance-level and class-level accuracy.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :param model: The trained PointNet++ model to be evaluated.' }],
                    [{ type: 'd', code: '    :param data_loader: DataLoader for the test dataset.' }],
                    [{ type: 'd', code: '    :param device: The device (CPU/GPU) where evaluation is executed.' }],
                    [{ type: 'd', code: '    :param num_classes: The number of unique classes in the dataset.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :return: `instance_acc` (float): Overall accuracy across all test samples. `class_acc` (float): Average accuracy across each class.' }],
                    [{ type: 'd', code: '    """' }],
                    
                    // Init vars
                    [{ type: 'b', code: '    total_correct = ' }, { type: 'h', code: '0' }, { type: 'e', code: '  # Accumulates the total number of predictions across all batches' }],
                    [{ type: 'b', code: '    total_seen = ' }, { type: 'h', code: '0' }, { type: 'e', code: '  # Accumulates the total number of instances across all batches' }],
                    [{ type: '', code: '' }],

                    // Init Arrays
                    [{ type: 'e', code: '    # Initialize NumPy arrays to track correct predictions and total instances per class, respectively' }],
                    [{ type: 'b', code: '    total_correct_class = np.zeros(num_classes)' }],
                    [{ type: 'b', code: '    total_seen_class = np.zeros(num_classes)' }],
                    [{ type: '', code: '' }],

                    // Model Eval
                    [{ type: 'b', code: '    classifier = model.eval()' }, { type: 'e', code: '  # Set model to evaluate' }],
                    [{ type: 'a', code: '    for' }, { type: 'b', code: ' batch_idx, (points, target) ' }, { type: 'a', code: 'in' }, { type: 'l', code: ' enumerate' }, { type: 'b', code: '(' }, { type: 'b', code: 'tqdm' }, { type: 'b', code: '(data_loader, ' }, { type: 'f', code: 'desc' }, { type: 'b', code: '=' }, { type: 'c', code: '"Evaluating"' }, { type: 'b', code: ')):' }],
                    [{ type: 'b', code: '        points, target = points.to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device), target.to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device)' }],
                    [{ type: '', code: '' }],
                    
                    [{ type: 'b', code: '        points = points.transpose(' }, { type: 'h', code: '2' }, { type: 'b', code: ', ' }, { type: 'h', code: '1' }, { type: 'b', code: ')' }],
                    [{ type: 'b', code: '        pred, _ = classifier(points)' }, { type: 'e', code: '  # Forward pass test data through the model' }],
                    [{ type: 'b', code: '        pred_choice = pred.data.max(' }, { type: 'h', code: '1' }, { type: 'b', code: ')[' }, { type: 'h', code: '1' }, { type: 'b', code: ']' }, { type: 'e', code: '  # Get predicted class with highest score' }],
                    [{ type: '', code: '' }],

                    // Instance Level Acc
                    [{ type: 'e', code: '        # Instance-Level Accuracy' }],
                    [{ type: 'b', code: '        correct = pred_choice.eq(target.long().data).cpu().sum()' }],
                    [{ type: 'b', code: '        total_correct += correct.item()' }],
                    [{ type: 'b', code: '        total_seen += points.size()[' }, { type: 'h', code: '0' }, { type: 'b', code: ']' }],
                    [{ type: '', code: '' }],

                    // Class Level Acc
                    [{ type: 'e', code: '        # Class-Level Accuracy' }],
                    [{ type: 'b', code: '        target_np = target.cpu().numpy()' }],
                    [{ type: 'b', code: '        pred_np = pred_choice.cpu().numpy()' }],
                    [{ type: 'a', code: '        for' }, { type: 'b', code: ' i ' }, { type: 'a', code: 'in' }, { type: 'l', code: ' range' }, { type: 'b', code: '(' }, { type: 'l', code: 'len' }, { type: 'b', code: '(target_np)):' }],
                    [{ type: 'b', code: '            label = target_np[i]' }],
                    [{ type: 'b', code: '            total_seen_class[label] += ' }, { type: 'h', code: '1' }],
                    [{ type: 'a', code: '            if' }, { type: 'b', code: ' pred_np[i] == label:' }],
                    [{ type: 'b', code: '                total_correct_class[label] += ' }, { type: 'h', code: '1' }],
                    [{ type: '', code: '' }],

                    // Compute Instance Acc
                    [{ type: 'e', code: '    # Compute Instance-Level Accuracy' }],
                    [{ type: 'b', code: '    instance_acc = ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(total_correct / total_seen)' }],
                    [{ type: '', code: '' }],

                    // Compute Class Acc
                    [{ type: 'e', code: '    # Compute per-class accuracy and average class accuracy' }],
                    [{ type: 'b', code: '    class_acc_per_class = total_correct_class / total_seen_class' }],
                    [{ type: 'b', code: '    class_acc = ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(np.mean(class_acc_per_class))' }],
                    [{ type: '', code: '' }],

                    // Print
                    [{ type: 'l', code: '    print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Test Instance Accuracy: " }, { type: 'a', code: '{' }, { type: 'b', code: 'instance_acc' }, { type: 'c', code: ':.3f' }, { type: 'a', code: '}' }, { type: 'c', code: ", Class Accuracy: " }, { type: 'a', code: '{' }, { type: 'b', code: 'class_acc' }, { type: 'c', code: ':.3f' }, { type: 'a', code: '}' }, { type: 'c', code: "'" }, { type: 'b', code: ')' }],
                    [{ type: '', code: '' }],

                    [{ type: 'a', code: '    return' }, { type: 'b', code: ' instance_acc, class_acc' }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'For each batch of data points fetched from the ' },
                    { type: 'span', className: 'code', data: 'test_data_loader' },
                    { type: 'Text', data: ', the model generates predictions through a single forward pass with argmax selection determining final class labels. The instance accuracy accumulates total correct predictions across all samples, while class accuracy separately tracks hits per category. This dual-metric approach addresses class imbalance by weighing each category equally in the final class accuracy calculation - computed as the mean of per-class hit rates rather than pooling all predictions. Final metrics provide both absolute performance (instance accuracy) and balanced category recognition assessment (class accuracy).' },
                ]
            },
            { type: 'Collapsable', colType: 'information', title: 'Initial evaluation with 1024 points', extraText: "For the evaluation part of this stage, I have just applied it to 1024 points and have not yet evaluated the models against non-uniform sampling. This will be applied in the next section." },

            // Fourth-Third Section - Model Checkpointing and Log Management
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'After the training and evaluation loop for each epoch is completed, if the current instance and class accuracy is greater than the best instance and class accuracy, then the model is saved with the label ' },
                    { type: 'span', className: 'code', data: 'best_model.pth' },
                    { type: 'Text', data: '. The current epochs model and its states are also saved with the label ' },
                    { type: 'span', className: 'code', data: 'current_model.pth' },
                    { type: 'Text', data: ' and overridden from the last epoch\'s one so that it can be used to continue training the model in case the user accidentally stops the training process. Finally, at the last epoch, the last trained model is trained separately with the label ' },
                    { type: 'span', className: 'code', data: 'final_model.pth' },
                    { type: 'Text', data: '. The data for all the training and evaluation are then logged into the final logger file, and the temporary logger file is then deleted. The best model, current model, and final model along with final logger file can be found in the folder ' },
                    { type: 'span', className: 'code', data: 'Logs/{your_folder_name}' },
                    { type: 'Text', data: '.' },
                ]
            },

            // Fifth Section - Model Evaluation
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To minimally test the model without changing the parameters, you can run the following shell command below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python test_classification.py --save_path your_folder_name --model PointNet2_CLS_SSG" }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The ' },
                    { type: 'span', className: 'code', data: 'test_classification' },
                    { type: 'Text', data: ' function in ' },
                    { type: 'span', className: 'code', data: 'test_classification.py' },
                    { type: 'Text', data: ' starts off similar as ' },
                    { type: 'span', className: 'code', data: 'train_classification' },
                    { type: 'Text', data: ' which is by selecting a computation device, CPU or GPU, to be used when testing the model. Next, logger files are then setup to tabulate all the results of the model. Then the ' },
                    { type: 'Link', to: "#third", className: 'blog_links', data: "data loading and preparation step" },
                    { type: 'Text', data: ' is initialized to load the testing data. It takes in the same save path you used for saving the trained models to load the best model checkpoint. The script then runs the ' },
                    { type: 'span', className: 'code', data: 'evaluate_model' },
                    { type: 'Text', data: ' function:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Python', extraText: [
                    // Function Definition
                    [{ type: 'a', code: 'def' }, { type: 'g', code: ' evaluate_model' }, { type: 'b', code: '(' }],
                    [{ type: 'b', code: '        model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, device: torch.device, num_classes: ' }, { type: 'l', code: 'int' }, { type: 'b', code: ', test_points: ' }, { type: 'l', code: 'int' }],
                    [{ type: 'b', code: ') -> ' }, { type: 'l', code: 'tuple' }, { type: 'b', code: '[' }, { type: 'l', code: 'float' }, { type: 'b', code: ', ' }, { type: 'l', code: 'float' }, { type: 'b', code: ']:' }],
                    [{ type: '', code: '' }],

                    // Docstring
                    [{ type: 'd', code: '    """' }],
                    [{ type: 'd', code: '    Evaluate the trained model on the test dataset.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :param model: The trained PointNet++ model.' }],
                    [{ type: 'd', code: '    :param data_loader: DataLoader for the test dataset.' }],
                    [{ type: 'd', code: '    :param device: The device (CPU/GPU) where evaluation is executed.' }],
                    [{ type: 'd', code: '    :param num_classes: The number of unique classes in the dataset.' }],
                    [{ type: 'd', code: '' }],
                    [{ type: 'd', code: '    :return: `instance_acc` (float): Overall accuracy across all test samples. `class_acc` (float): Average accuracy across each class.' }],
                    [{ type: 'd', code: '    """' }],
                    
                    // Init Vars
                    [{ type: 'b', code: '    total_correct = ' }, { type: 'h', code: '0' }, { type: 'e', code: '  # Accumulates the total number of predictions across all batches' }],
                    [{ type: 'b', code: '    total_seen = ' }, { type: 'h', code: '0' }, { type: 'e', code: '  # Accumulates the total number of instances across all batches' }],
                    [{ type: '', code: '' }],

                    // Init Arrays
                    [{ type: 'e', code: '    # Initialize NumPy arrays to track correct predictions and total instances per class, respectively' }],
                    [{ type: 'b', code: '    total_correct_class = np.zeros(num_classes)' }],
                    [{ type: 'b', code: '    total_seen_class = np.zeros(num_classes)' }],
                    [{ type: '', code: '' }],

                    // Outer Loop
                    [{ type: 'b', code: '    classifier = model.eval()' }, { type: 'e', code: '  # Set model to evaluate' }],
                    [{ type: 'a', code: '    for' }, { type: 'b', code: ' points, target ' }, { type: 'a', code: 'in' }, { type: 'b', code: ' tqdm(' }, { type: 'b', code: 'data_loader, ' }, { type: 'f', code: 'total' }, { type: 'b', code: '=' }, { type: 'l', code: 'len' }, { type: 'b', code: '(data_loader)):' }],
                    [{ type: 'b', code: '        batch_pred_sum = torch.zeros(target.size()[' }, { type: 'h', code: '0' }, { type: 'b', code: '], num_classes).to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device)'}, { type: 'e', code: '  # score for classes' }],
                    [{ type: '', code: '' }],

                    // Vote Loop
                    [{ type: 'a', code: '        for' }, { type: 'b', code: ' vote_idx ' }, { type: 'a', code: 'in' }, { type: 'l', code: ' range' }, { type: 'b', code: '(args.num_votes):' }],
                    [{ type: 'e', code: '            # Shuffle point order to achieve different farthest samplings' }],
                    [{ type: 'b', code: '            shuffled_indices = np.arange(args.num_points)' }],
                    [{ type: 'b', code: '            np.random.shuffle(shuffled_indices)' }],
                    [{ type: 'b', code: '            points = points[:, shuffled_indices, :]' }],
                    [{ type: '', code: '' }],
                    
                    [{ type: 'e', code: '            # Calculate rotation angle and rotate point clouds' }],
                    [{ type: 'b', code: '            rotation_angle = vote_idx / ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(args.num_votes) * np.pi * ' }, { type: 'h', code: '2' }],
                    [{ type: 'b', code: '            rotated_data = AugmentData(' }, { type: 'f', code: 'batch_data' }, { type: 'b', code: '=points, ' }, { type: 'f', code: 'use_normals' }, { type: 'b', code: '=args.use_normals).rotate_point_cloud_by_angle(' }, { type: 'f', code: 'data' }, { type: 'b', code: '=points, ' }, { type: 'f', code: 'rotation_angle' }, { type: 'b', code: '=rotation_angle)' }],
                    [{ type: '', code: '' }],

                    // Dropout Logic
                    [{ type: 'e', code: '            # Random input dropout for testing on dataset with non uniform sampling density' }],
                    [{ type: 'b', code: '            dropout_ratio = ' }, { type: 'h', code: '1' }, { type: 'b', code: ' - ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(test_points / args.num_points)' }],
                    [{ type: 'b', code: '            feed_data = AugmentData(' }, { type: 'f', code: 'batch_data' }, { type: 'b', code: '=torch.from_numpy(rotated_data), ' }, { type: 'f', code: 'use_normals' }, { type: 'b', code: '=args.use_normals).random_point_dropout(' }, { type: 'f', code: 'data' }, { type: 'b', code: '=rotated_data, ' }, { type: 'f', code: 'max_dropout_ratio' }, { type: 'b', code: '=dropout_ratio, ' }, { type: 'f', code: 'mode' }, { type: 'b', code: '=' }, { type: 'c', code: "'test'" }, { type: 'b', code: ')' }],
                    [{ type: 'b', code: '            feed_data = torch.from_numpy(feed_data)' }],
                    [{ type: '', code: '' }],

                    // Processing & Forward
                    [{ type: 'e', code: '            # Original shape: (batch_size, num_features, num_points), New shape: (batch_size, num_points, num_features)' }],
                    [{ type: 'b', code: '            feed_data = feed_data.transpose(' }, { type: 'h', code: '2' }, { type: 'b', code: ', ' }, { type: 'h', code: '1' }, { type: 'b', code: ')' }],
                    [{ type: 'b', code: '            feed_data, target = feed_data.to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device), target.to(' }, { type: 'f', code: 'device' }, { type: 'b', code: '=device)' }],
                    [{ type: '', code: '' }],
                    [{ type: 'b', code: '            pred, _ = classifier(feed_data)' }, { type: 'e', code: '  # Forward pass test data through the model' }],
                    [{ type: 'b', code: '            batch_pred_sum += pred' }, { type: 'e', code: '  # Accumulate predictions' }],
                    
                    // Aggregation
                    [{ type: 'b', code: '        pred = batch_pred_sum / args.num_votes' }, { type: 'e', code: '  # Averaging the accumulated votes' }],
                    [{ type: 'b', code: '        pred_choice = pred.data.max(' }, { type: 'h', code: '1' }, { type: 'b', code: ')[' }, { type: 'h', code: '1' }, { type: 'b', code: ']' }, { type: 'e', code: '  # Get predicted class with highest score' }],
                    [{ type: '', code: '' }],

                    // Update Metrics
                    [{ type: 'e', code: '        # Update overall correct predictions and total seen' }],
                    [{ type: 'b', code: '        correct = pred_choice.eq(target.long().data).cpu().sum().item()' }],
                    [{ type: 'b', code: '        total_correct += correct' }],
                    [{ type: 'b', code: '        total_seen += points.size()[' }, { type: 'h', code: '0' }, { type: 'b', code: ']' }],
                    [{ type: '', code: '' }],

                    [{ type: 'e', code: '        # Update per-class correct predictions and total seen per class' }],
                    [{ type: 'b', code: '        target_np = target.cpu().numpy()' }],
                    [{ type: 'b', code: '        pred_np = pred_choice.cpu().numpy()' }],
                    [{ type: 'a', code: '        for' }, { type: 'b', code: ' i ' }, { type: 'a', code: 'in' }, { type: 'l', code: ' range' }, { type: 'b', code: '(' }, { type: 'l', code: 'len' }, { type: 'b', code: '(target_np)):' }],
                    [{ type: 'b', code: '            label = target_np[i]' }],
                    [{ type: 'b', code: '            total_seen_class[label] += ' }, { type: 'h', code: '1' }],
                    [{ type: 'a', code: '            if' }, { type: 'b', code: ' pred_np[i] == label:' }],
                    [{ type: 'b', code: '                total_correct_class[label] += ' }, { type: 'h', code: '1' }],
                    [{ type: '', code: '' }],

                    // Final Calc
                    [{ type: 'e', code: '    # Compute overall accuracy' }],
                    [{ type: 'b', code: '    instance_acc = ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(total_correct / total_seen)' }],
                    [{ type: '', code: '' }],
                    [{ type: 'e', code: '    # Compute per-class accuracy and average class accuracy' }],
                    [{ type: 'b', code: '    class_acc_per_class = total_correct_class / total_seen_class' }],
                    [{ type: 'b', code: '    class_acc = ' }, { type: 'l', code: 'float' }, { type: 'b', code: '(np.mean(class_acc_per_class))' }],
                    [{ type: '', code: '' }],

                    // Print
                    [{ type: 'l', code: '    print' }, { type: 'b', code: '(' }, { type: 'c', code: "f'Number of Test Points: " }, { type: 'a', code: '{' }, { type: 'b', code: 'test_points' }, { type: 'a', code: '}' }, { type: 'c', code: ", Test Instance Accuracy: " }, { type: 'a', code: '{' }, { type: 'b', code: 'instance_acc' }, { type: 'c', code: ':.3f' }, { type: 'a', code: '}' }, { type: 'c', code: ", Class Accuracy: " }, { type: 'a', code: '{' }, { type: 'b', code: 'class_acc' }, { type: 'c', code: ':.3f' }, { type: 'a', code: '}' }, { type: 'c', code: "'" }, { type: 'b', code: ')' }],
                    [{ type: 'a', code: '    return' }, { type: 'b', code: ' instance_acc, class_acc' }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'This code defines a function ' },
                    { type: 'span', className: 'code', data: 'evaluate_model' },
                    { type: 'Text', data: ' that evaluates a trained PointNet++ model on a test dataset. Similar to the ' },
                    { type: 'span', className: 'code', data: 'evaluate_one_epoch' },
                    { type: 'Text', data: ' function, it computes instance accuracy (overall accuracy across all samples) and class accuracy (average accuracy across each class) to address class imbalance. However, it introduces a voting mechanism to enhance predictions. For each batch of data, the function performs multiple forward passes ' },
                    { type: 'span', className: 'code', data: 'args.num_votes' },
                    { type: 'Text', data: ' with different augmentations: the order of points is shuffled to simulate diverse farthest point samplings, and each point cloud is rotated by a unique angle to generate varied views, and a random input dropout is applied to simulate non-uniform sampling densities. This dropout mechanism calculates a ratio based on the target test points versus the original point count, randomly discarding points to evaluate the models robustness on sparser data. Predictions from all votes are accumulated and averaged to produce the final prediction for each sample. The same dual-metric approach is used for evaluating the model. In other words, you ask the model '},
                    { type: 'span', className: 'code', data: 'args.num_votes'},
                    { type: 'Text', data: ' times "What is this?" at different angles (rotates/shuffles) and take the average to get the highest possible score. The selected model is tested on varying non-uniform point cloud density where the number of points is from ' },
                    { type: 'span', className: 'code', data: '1024' },
                    { type: 'Text', data: ' to ' },
                    { type: 'span', className: 'code', data: '128' },
                    { type: 'Text', data: ' with intervals of ' },
                    { type: 'span', className: 'code', data: '16' },
                    { type: 'Text', data: ' points. While this level of granularity might seem excessive, it provides a high-resolution visualization of the model\'s robustness. The data for evaluation results are logged into a logger file and can be found in ' },
                    { type: 'span', className: 'code', data: 'Logs/{your_folder_name}/log_test_final.txt' },
                    { type: 'Text', data: '.' },
                ]
            },
            { type: 'Collapsable', colType: 'warning', title: 'Warning', extraText: "Please be aware that if any value in the number of testing points is larger than the number of points each point cloud was trained with, this is not allowed. The test pipeline can only drop points to simulate fewer points. If test points exceed training points, the computed dropout ratio becomes negative, and the pipeline cannot \"create\" additional points beyond what exists in the input point cloud" },

            // Fifth-First Section: Robustness to Non-Uniform Point Density
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To evaluate the robustness of the trained models, each variant was tested against varying levels of point cloud density. This replicates the "density adaptive" experiment from the original PointNet++ paper. I evaluated the models on point counts ranging from ' },
                    { type: 'span', className: 'code', data: '1024' },
                    { type: 'Text', data: ' down to ' },
                    { type: 'span', className: 'code', data: '128' },
                    { type: 'Text', data: ' (decreasing by intervals of ' },
                    { type: 'span', className: 'code', data: '16' },
                    { type: 'Text', data: '). Graph below illustrates the results with the y-axis scaled from ' },
                    { type: 'span', className: 'code', data: '0%' },
                    { type: 'Text', data: ' to ' },
                    { type: 'span', className: 'code', data: '100%' },
                    { type: 'Text', data: ' to fully capture the performance drop-off. This is built in ' },
                    { type: 'span', className: 'code', data: 'ReactJS' },
                    { type: 'Text', data: ' using the ' },
                    { type: 'span', className: 'code', data: 'plotly.js' },
                    { type: 'Text', data: ' library for rendering the 2D line plot.'}
                ]
            },
            { type: 'unique_code', className: 'BlogPage2Data_RobustnessPlot', data: <RobustnessPlot /> },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The results from the plot above vividly demonstrate the critical importance of random input dropout during training. The models trained without dropout, SSG (light pink), MRG (light blue), and MSG (light green), suffer a catastrophic performance collapse as the point density decreases. While they start with high accuracy at ' },
                    { type: 'span', className: 'code', data: '1024' },
                    { type: 'Text', data: ' points, their performance plummets to near-zero accuracy as the number of points drops below ' },
                    { type: 'span', className: 'code', data: '400' },
                    { type: 'Text', data: '. This indicates that without dropout, the network overfits to the specific dense point structure and fails completely when data becomes sparse.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'In stark contrast, the models trained with dropout, SSG+DP (dark red), MRG+DP (dark blue), and MSG+DP (dark green), exhibit remarkable resilience. Instead of collapsing, they show a graceful degradation in performance. Most notably, the MSG+DP model proves to be the most robust, maintaining over ' },
                    { type: 'span', className: 'code', data: '80%' },
                    { type: 'Text', data: ' accuracy even when the point count is reduced to just ' },
                    { type: 'span', className: 'code', data: '128' },
                    { type: 'Text', data: ' points. This validates the effectiveness of the MSG architecture combined with density adaptive training, ensuring reliable classification even with significantly sparse or non-uniform sensor data.' },
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'You can generate this plot by running '},
                    { type: 'span', className: 'code', data: 'visualizers/Robustness_Plot.py' },
                    { type: 'Text', data: ' script which will load all the saved testing logs from the ' },
                    { type: 'span', className: 'code', data: 'Blog_Logs/{your_folder_name}/' },
                    { type: 'Text', data: ' directory. The script parses the ' },
                    { type: 'span', className: 'code', data: 'log_test_final.txt' },
                    { type: 'Text', data: ' file for each model variant, extracts the accuracy data corresponding to specific point densities, and uses '},
                    { type: 'span', className: 'code', data: 'plotly' },
                    { type: 'Text', data: ' Python library to render the comparison graph. The shell command below can be used to do this' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python -m visualizers.Robustness_Plot --use_blog_logs" }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To better understand its specific performance on the machining features, I visualized the results using additional methods: t-Distributed Stochastic Neighbor Embedding (t-SNE) feature embedding, confusion matrix, and time taken plot to visualize how long it takes to train each of the models.' },
                ]
            },

            // Fifth-Second Section: t-SNE Feature Embedding
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To visualize how the model "sees" the data, I used t-SNE to project the high-dimensional features extracted by the PointNet++ model into a 3D space. The GUI below shows an interactive visual of t-SNE. The user is given a choice to select 6 different model variants and 8 different non-uniform point densities. This is also built in ' },
                    { type: 'span', className: 'code', data: 'ReactJS' },
                    { type: 'Text', data: ' using the ' },
                    { type: 'span', className: 'code', data: 'plotly.js' },
                    { type: 'Text', data: ' library for rendering the 3D scatter plot. Each point in the plot represents a point cloud from the test set, colored according to its true class label. Clusters of points indicate that the model is effectively grouping similar classes together based on learned features. The t-SNE visualizations reveal several insights about the models performance. For instance, models trained with random input dropout exhibit more distinct and well-separated clusters, indicating better feature learning and class discrimination. In contrast, models without dropout show more overlap between classes, suggesting confusion in feature representation. Additionally, as point density decreases, the clusters become less defined, highlighting the challenges posed by sparse data. Overall, these visualizations provide a qualitative assessment of how different training strategies impact the learned feature space and the models ability to generalize across varying input conditions.' },
                ]
            },
            { type: 'unique_code', className: 'BlogPage2Data_TSNE', data: <TSNE /> },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'Unless I set up a backend server, uploaded all the models, and configured an API to handle inference requests, running this in real-time would be impossible on a static site. Alas, I am not yet capable of doing such things, therefore, I opted to pre-compute the t-SNE embeddings locally and save the output as a JSON file, which the website simply loads and renders. To generate this data, I created a script ' },
                    { type: 'span', className: 'code', data: 'visualizers/TSNE_Plot.py' },
                    { type: 'Text', data: '. This script iterates through the test dataset and extracts the models features for every object. To perform the actual dimensionality reduction, I utilized the ' },
                    { type: 'span', className: 'code', data: 'TSNE' },
                    { type: 'Text', data: ' module from the ' },
                    { type: 'span', className: 'code', data: 'scikit-learn' },
                    { type: 'Text', data: ' Python library. It takes the high-dimensional feature vectors extracted by the model and projects them into a 3-dimensional space, generating the coordinate data required for the visualization. I ran this script for all 6 model variants and across 8 different non-uniform point densities (from ' },
                    { type: 'span', className: 'code', data: '128'},
                    { type: 'Text', data: ' to ' },
                    { type: 'span', className: 'code', data: '1024' },
                    { type: 'Text', data: ' points with intervals of ' },
                    { type: 'span', className: 'code', data: '128' },
                    { type: 'Text', data: ' points). You can also use the blog logs to visualize the t-SNE plot using the shell comand below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python -m visualizers.TSNE_Plot --save_path MSG_Dropout --model PointNet2_CLS_MSG --num_points_test 1024 --use_blog_logs" }],
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The main difference between '},
                    { type: 'span', className: 'code', data: 'visualizers/TSNE_Plot.py' },
                    { type: 'Text', data: ' and '},
                    { type: 'span', className: 'code', data: 'test_classification.py' },
                    { type: 'Text', data: ' is that when calculating the official accuracy of the model, we want the most robust prediction possible. To achieve this, the testing script utilizes a "voting" system (set by ' },
                    { type: 'span', className: 'code', data: '--num_votes 12' },
                    { type: 'Text', data: '). It takes a point cloud, rotates it 12 times around the vertical axis, and passes each rotation through the model. The final classification is the average of these 12 predictions. For the visualization, we want to see exactly  how the model interprets a specific, static instance of a point cloud. Therefore, this script runs a single forward pass without any rotational augmentation or voting. This provides a raw "snapshot" of where that specific point cloud sits in the model\'s learned feature space, without smoothing out the results via averaging. The resulting 3D coordinates, along with their predicted labels and class names, were saved into a JSON format, allowing the interactive plot above to switch smoothly between models and densities.'}
                ]
            },
            
            // Fifth-Third Section: Confusion Matrix
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To further analyze the classification performance of each model variant, I generated confusion matrices that illustrate how well the models distinguish between different classes. A confusion matrix is a table that summarizes the performance of a classification algorithm by displaying the counts of true positive, false positive, true negative, and false negative predictions for each class. In this case, each row of the matrix represents the actual class labels from the test dataset, while each column represents the predicted class labels output by the model. The diagonal elements indicate correct classifications, while off-diagonal elements reveal misclassifications. By examining these matrices, we can identify which classes are frequently confused with one another, providing insights into potential weaknesses in the model\'s feature learning. For instance, if certain classes consistently show high misclassification rates, it may indicate that their features are not sufficiently distinct in the learned representation space. This analysis helps in understanding the strengths and limitations of each model variant. Similar to the t-SNE GUI above, the user is given an option to select 6 different model variants and 8 different non-uniform point densities. This is also built in ' },
                    { type: 'span', className: 'code', data: 'ReactJS' },
                    { type: 'Text', data: ' using the ' },
                    { type: 'span', className: 'code', data: 'plotly.js' },
                    { type: 'Text', data: ' library for rendering the 2D heatmap.' },
                ]
            },
            { type: 'unique_code', className: 'BlogPage2Data_CM', data: <ConfusionMatrix /> },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The confusion matrix is generated in the same way as above, by using ' },
                    { type: 'span', className: 'code', data: 'confusion_matrix' },
                    { type: 'Text', data: ' module from the ' },
                    { type: 'span', className: 'code', data: 'scikit-learn' },
                    { type: 'Text', data: ' Python library for each of the model variants and different non-uniform point densities and are saved to a JSON file which are then later read to get rendered in this blog post. The script for this can be found in ' },
                    { type: 'span', className: 'code', data: 'visualizers/ConfusionMatrix_Plot.py' },
                    { type: 'Text', data: '. Similarly, using the blog logs to visualize the confusion matrix plot by running the shell command below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python -m visualizers.ConfusionMatrix_Plot --save_path MSG_Dropout --model PointNet2_CLS_MSG --num_points_test 1024 --use_blog_logs" }],
                ]
            },

            // Fifth-Fourth Section: Training Time Analysis
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The PointNet++ paper ' },
                    { type: 'Link', to: '#ref_1', className: 'blog_links', data: [{ type: 'Text', data: '[1]' }] },
                    { type: 'Text', data: 'claims that the MRG approach is computationally more efficient than MSG. To verify this claim, I tracked the training time and test accuracy for all six model variants across 251 epochs. The plot below illustrates the convergence speed of each model, mapping the test accuracy against the cumulative training time in hours.'},
                ]
            },
            { type: 'unique_code', className: 'BlogPage2Data_RobustnessPlot', data: <TimeTaken /> },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'The data reveals a clear picture of the computational cost. MRG+DP took the least amount of time to train, completing in approximately ' },
                    { type: 'span', className: 'code', data: '14.45 hours' },
                    { type: 'Text', data: ' and reaching a final accuracy of ' },
                    { type: 'span', className: 'code', data: '94.60%' },
                    { type: 'Text', data: '. This was faster even than the SSG models: SSG+DP followed at ' },
                    { type: 'span', className: 'code', data: '15.24 hours' },
                    { type: 'Text', data: ' (' },
                    { type: 'span', className: 'code', data: '95.10%' },
                    { type: 'Text', data: ' accuracy), with MRG taking ' },
                    { type: 'span', className: 'code', data: '15.33 hours' },
                    { type: 'Text', data: ' (' },
                    { type: 'span', className: 'code', data: '97.50%' },
                    { type: 'Text', data: ' accuracy) and SSG taking ' },
                    { type: 'span', className: 'code', data: '15.77 hours' },
                    { type: 'Text', data: ' (' },
                    { type: 'span', className: 'code', data: '97.70%' },
                    { type: 'Text', data: ' accuracy). The MSG models, as predicted by the authors, were significantly more expensive. MSG required ' },
                    { type: 'span', className: 'code', data: '24.40 hours' },
                    { type: 'Text', data: ' to reach its high accuracy of ' },
                    { type: 'span', className: 'code', data: '98.50%' },
                    { type: 'Text', data: ', with MSG+DP being the most computationally intensive, taking the longest at ' },
                    { type: 'span', className: 'code', data: '25.61 hours' },
                    { type: 'Text', data: ' (' },
                    { type: 'span', className: 'code', data: '97.30%' },
                    { type: 'Text', data: ' accuracy). While MSG+DP is the most expensive model, the robustness analysis in the ' },
                    { type: 'Link', to: "#fifth_first", className: 'blog_links', data: "previous section" },
                    { type: 'Text', data: ' demonstrates that this extra cost yields significant gains in stability against non-uniform density. MRG offers a middle ground, it is slightly faster than MSG while offering better robustness than SSG. All of these models were trained and tested on ' },
                    { type: 'span', className: 'code', data: 'Nvidia RTX 4080' },
                    { type: 'Text', data: ' GPU, ' },
                    { type: 'span', className: 'code', data: '13th Gen Intel(R) Core(TM) i7-13700K' },
                    { type: 'Text', data: ' CPU, with ' },
                    { type: 'span', className: 'code', data: '32 GB DDR5' },
                    { type: 'Text', data: ' RAM.' },

                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'Similarly, you can generate this plot by running '},
                    { type: 'span', className: 'code', data: 'visualizers/TimeTaken_Plot.py' },
                    { type: 'Text', data: ' script which will load all the saved training logs from the ' },
                    { type: 'span', className: 'code', data: 'Blog_Logs' },
                    { type: 'Text', data: ' directory. The script parses the ' },
                    { type: 'span', className: 'code', data: 'log_train_final.txt' },
                    { type: 'Text', data: ' file for each model variant, extracts the test accuracy data corresponding to specific time it finishes training the epoch, and uses '},
                    { type: 'span', className: 'code', data: 'plotly' },
                    { type: 'Text', data: ' Python library to render the graph. The shell command below can be used to do this' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python -m visualizers.TimeTaken_Plot --use_blog_logs" }],
                ]
            },
            

            // Sixth Section - PCD Visualizer
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'To help visualize some of the concepts in this blog post, I have created visualizer classes. These classes allow you to select a random STL file or a specific STL file from the dataset folder ' },
                    { type: 'span', className: 'code', data: 'MFD_dataset/raw' },
                    { type: 'Text', data: ' and convert them into a user specified number of PCD points by using a slider between the range of user specified minimum and maximum number of points. Depending on the type of visualization and the settings/parameters, visualization window will be different. These visualizers are created using '},
                    { type: 'span', className: 'code', data: 'PySide6' },
                    { type: 'Text', data: ' for the GUI and ' },
                    { type: 'span', className: 'code', data: 'open3d' },
                    { type: 'Text', data: ' embedded inside for rendering and processing PCD. The available visualizer classes and their settings/parameters for this blog will be explained in the sections below.' },
                ]
            },

            // Sixth-First Section - Preprocessing Visualizer
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Link', to: "#fig_3", className: 'blog_links', data: "Figure 3" },
                    { type: 'Text', data: ' shows the visualizer class used for preprocessing PCD data. You can play around with the number of PCD points you want to sample from the STL file. This visualizer allows you to:'},
                ]
            },
            { 
                type: 'ul', className: 'body', data: [
                    { type: 'li', data: [{ type: 'Text', data: 'Estimate normals of PCD by specifying the radius and maximum nearest neighbors for the search parameters of '}, { type: 'span', className: 'code', data: 'KDTreeSearchParamHybrid(radius=r, max_nn=n)' }] },
                    { type: 'li', data: [{ type: 'Text', data: 'Option to orient normals consistently by specifying '}, { type: 'span', className: 'code', data: 'k' }, { type: 'Text', data: ' (number of k nearest neighbours), if the estimate normals option is checked'}] },
                    { type: 'li', data: [{ type: 'Text', data: 'Visualize normals calculations of a single point picked from PCD, where green wireframe sphere represents the radius, the colored blue points represents the number of k nearest neighbors (less than '}, { type: 'span', className: 'code', data: 'max_nn' }, { type: 'Text', data: ') of the search parameter'}, { type: 'span', className: 'code', data: 'KDTreeSearchParamHybrid'}, { type: 'Text', data: ').'}] },
                    { type: 'li', data: [{ type: 'Text', data: 'Apply FPS by selecting the number of points to FPS sample between the minimum points (100 points) and maximum of the total number of user specified PCD points. Since the FPS points will be used as centroids, their position is also highlighted as red in the original PCD visualizer window on the left while the original PCD points are grayed out.'}]}
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'You can refer to ' },
                    { type: 'Link', to: "#second", className: 'blog_links', data: "this section" },
                    { type: 'Text', data: ' for more information regarding preprocessing of PCD. You can run this visualizer script by running the shell command below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python -m visualizers.Preprocessing_PCD" }],
                ]
            },
            { type: 'img', id: 'fig_3', className: 'Blog_Image', data: [Figure3, 'Figure 3: PCD Preprocessing Visualizer', ''] },


            // Sixth-Second Section - Data Augmentation Visualizer
            { type: 'span', className: 'section_title sectionIcon', id: sectionLinks.shift(), icon: faSection, data: tocTitles.shift() },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Link', to: "#fig_4", className: 'blog_links', data: "Figure 4" },
                    { type: 'Text', data: ' shows the visualizer class used for data augmentation of PCD points during the training/testing process. You can refer to this section to understand more about '},
                    { type: 'Link', to: "#fourth_first", className: 'blog_links', data: "PCD data augmentation" },
                    { type: 'Text', data: '. This visualizer allows you to:'},
                ]
            },
            {
                type: 'ul', className: 'body', data: [
                    { type: 'li', data: [{ type: 'Text', data: 'Estimate normals of PCD by specifying the radius and maximum nearest neighbours for the search parameter with an option to orient normals consistently by specifying number of k nearest neighbours (similar to '}, { type: 'Link', to: "#sixth_first", className: 'blog_links', data: "section above" }, { type: 'Text', data: ')'}] },
                    { type: 'li', data: [{ type: 'Text', data: 'Rotate PCD (with an option to rotate its normals)'}]},
                    { type: 'li', data: [{ type: 'Text', data: 'Rotate Perturbation PCD (with an option to rotate its normals). The user can specify angle '}, { type: 'span', className: 'code', data: ''}, { type: 'Text', data: ' and clipping angle'}]},
                    { type: 'li', data: [{ type: 'Text', data: 'Scale PCD by specifying a range for randomizing the scale value'}]},
                    { type: 'li', data: [{ type: 'Text', data: 'Shift PCD by specifying a range for randomizing the shift value'}]},
                    { type: 'li', data: [{ type: 'Text', data: 'Jitter PCD by specifying the clipping value for the Gaussian noise (STD)'}]},
                    { type: 'li', data: [{ type: 'Text', data: 'Shuffle PCD (for visualizing purposes, the colors are shuffled instead of the points)'}]},
                    { type: 'li', data: [{ type: 'Text', data: 'Randomly dropout PCD based on the maximum ratio of points to drop from each PCD sample. A random dropout percentage between '}, { type: 'span', className: 'code', data: '0' }, { type: 'Text', data: ' and '}, { type: 'span', className: 'code', data: 'max_dropout_ratio' }, { type: 'Text', data: ' is chosen per batch. Default is '}, { type: 'span', className: 'code', data: '0.95' }, { type: 'Text', data: ' (drops up to '}, { type: 'span', className: 'code', data: '95%' }, { type: 'Text', data: ' of points in extreme cases)'}]},
                ]
            },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'A coordinate frame is added to the visualizer, where the ' },
                    { type: 'span', className: 'code', data: 'XYZ' },
                    { type: 'Text', data: ' axes are represented by red, green, and blue arrows respectively. That is so that it\'s easy to understand the augmented vs original PCD. You can run this visualizer script by running the shell command below:' },
                ]
            },
            {
                type: 'CodeBlock', title: 'Shell', extraText: [
                    [{ type: 'shell', code: "python -m visualizers.Augmentation" }],
                ]
            },
            { type: 'img', id: 'fig_4', className: 'Blog_Image', data: [Figure4, 'Figure 4: PCD Data Augmentation Visualizer', ''] },


            // Conclusion Section
            { type: 'span', className: 'section_title sectionIcon', id: 'conclusion', icon: faSection, data: 'Conclusion' },
            {
                type: 'span', className: 'body', data: [
                    { type: 'Text', data: 'This has been a very interesting project for me to work on, presenting a full end-to-end workflow for training a custom PointNet++ classifier on the Machining Feature Dataset. As of writing this, I actually havent come across any other code repositories that implement the MRG classification model. Feel free to clone the repo, break the code, and experiment with your own parameters. If you have any questions or run into any bugs, do open an issue on the GitHub repository. Currently, I have no immediate plans to continue extending this workflow, but maybe in the future, I might visit the segmentation part of PointNet++ or compare it to other similar point cloud architectures to see how they stack up.' }
                ]
            },
            
            // References
            { type: 'span', className: 'section_title reference_section_title sectionIcon', id: 'references', icon: faSection, data: 'References' },
            referencesContent,

            // Footer
            footer_content({ BlogIndex: BlogIndex, previous_link: false, next_link: true })
        ]
    }
]

// Add subtitle content (published date, word count, reading time, and folder name)
blogpage_subtitle_content({ BlogIndex: BlogIndex, BlogPageData: BlogPage2Data })

export default BlogPage2Data;